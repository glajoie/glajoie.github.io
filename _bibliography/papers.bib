---
---

@inproceedings{vonoswald2025mesanetsequencemodelinglocally,
	title        = {MesaNet: Sequence Modeling by Locally Optimal Test-Time Training},
	author       = {Johannes von Oswald and Nino Scherrer and Seijin Kobayashi and Luca Versari and Songlin Yang and Maximilian Schlegel and Kaitlin Maile and Yanick Schimpf and Oliver Sieberling and Alexander Meulemans and Guillaume Lajoie and Rif A. Saurous and Charlotte Frenkel and Razvan Pascanu and Blaise Agüera y Arcas and João Sacramento},
	year         = 2026,
	booktitle    = {The Fourteenth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=xa3OnTb6c3},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@misc{kobayashi2025emergent,
	title        = {Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning},
	author       = {Seijin Kobayashi and Yanick Schimpf and Maximilian Schlegel and Angelika Steger and Maciej Wo\l{}czyk and Johannes von Oswald and Nino Scherrer and Kaitlin Maile and Guillaume Lajoie and Blake A. Richards and Rif A. Saurous and James Manyika and Blaise Ag\"{u}era y Arcas and Alexander Meulemans and Jo\~{a}o Sacramento},
	year         = 2025,
	url          = {https://arxiv.org/abs/2512.20605},
	eprint       = {2512.20605},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{ryoo2025generalizable,
	title        = {Generalizable, real-time neural decoding with hybrid state-space models},
	author       = {Avery Hee-Woon Ryoo and Nanda H. Krishna and Ximeng Mao and Mehdi Azabou and Eva L. Dyer and Matthew G. Perich and Guillaume Lajoie},
	year         = 2025,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 38,
	url          = {https://openreview.net/forum?id=1i4wNFgHDd},
	abbr         = {NeurIPS},
	bibtex_show  = {true}
}

@inproceedings{li2025tracing,
	title        = {Tracing the Representation Geometry of Language Models from Pretraining to Post-training},
	author       = {Melody Zixuan Li and Kumar Krishna Agrawal and Arna Ghosh and Komal Kumar Teru and Adam Santoro and Guillaume Lajoie and Blake A. Richards},
	year         = 2025,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 38,
	url          = {https://openreview.net/forum?id=FDruZlKWUb},
	abbr         = {NeurIPS},
	bibtex_show  = {true}
}

@inproceedings{krishna2025towards,
	title        = {Towards a generalizable, unified framework for multimodal neural decoding},
	author       = {Nanda H Krishna and Mathys Loiselle and Avery Hee-Woon Ryoo and Matthew G. Perich and Guillaume Lajoie},
	year         = 2025,
	booktitle    = {NeurIPS 2025 Workshop on Foundation Models for the Brain and Body},
	url          = {https://openreview.net/forum?id=ZMRxom1HKo},
	abbr         = {NeurIPS},
	bibtex_show  = {true}
}

@inproceedings{jamkhandi2025inferring,
	title        = {Inferring dynamical features from neural data through joint learning of latent factors and weights},
	author       = {Anirudh Gururaj Jamkhandi and Ali Korojy and Olivier Codol and Guillaume Lajoie and Matthew G. Perich},
	year         = 2025,
	booktitle    = {NeurIPS 2025 Workshop on Symmetry and Geometry in Neural Representations},
	url          = {https://openreview.net/forum?id=Y4dulSURZ1},
	abbr         = {NeurIPS},
	bibtex_show  = {true}
}

@article{paugam2025training,
	title        = {Training neural networks from scratch in a videogame leads to brittle brain encoding},
	author       = {Paugam, Fran\c{c}ois and Pinsard, Basile and St-Laurent, Marie and Lajoie, Guillaume and Bellec, Lune},
	year         = 2025,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2025.11.28.691119},
	url          = {https://www.biorxiv.org/content/early/2025/12/02/2025.11.28.691119},
	abbr         = {bioRxiv},
	elocation-id = {2025.11.28.691119},
	bibtex_show  = {true}
}

@misc{meulemans2025embedded,
	title        = {Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning},
	author       = {Alexander Meulemans and Rajai Nasser and Maciej Wo\l{}czyk and Marissa A. Weis and Seijin Kobayashi and Blake Richards and Guillaume Lajoie and Angelika Steger and Marcus Hutter and James Manyika and Rif A. Saurous and Jo\~{a}o Sacramento and Blaise Ag\"{u}era y Arcas},
	year         = 2025,
	url          = {https://arxiv.org/abs/2511.22226},
	eprint       = {2511.22226},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@misc{mittal2025iterative,
	title        = {Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers},
	author       = {Sarthak Mittal and Divyat Mahajan and Guillaume Lajoie and Mohammad Pezeshki},
	year         = 2025,
	url          = {https://arxiv.org/abs/2510.11471},
	eprint       = {2510.11471},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@misc{venkatraman2025recursive,
	title        = {Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models},
	author       = {Siddarth Venkatraman and Vineet Jain and Sarthak Mittal and Vedant Shah and Johan Obando-Ceron and Yoshua Bengio and Brian R. Bartoldson and Bhavya Kailkhura and Guillaume Lajoie and Glen Berseth and Nikolay Malkin and Moksh Jain},
	year         = 2025,
	url          = {https://arxiv.org/abs/2509.26626},
	eprint       = {2509.26626},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{elmoznino2025towards,
	title        = {Towards a Formal Theory of Representational Compositionality},
	author       = {Elmoznino, Eric and Jiralerspong, Thomas and Bengio, Yoshua and Lajoie, Guillaume},
	year         = 2025,
	month        = {13--19 Jul},
	booktitle    = {Proceedings of the 42nd International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 267,
	pages        = {15266--15295},
	url          = {https://proceedings.mlr.press/v267/elmoznino25a.html},
	abbr		 = {ICML},
	editor	     = {Singh, Aarti and Fazel, Maryam and Hsu, Daniel and Lacoste-Julien, Simon and Berkenkamp, Felix and Maharaj, Tegan and Wagstaff, Kiri and Zhu, Jerry},
	pdf		     = {https://raw.githubusercontent.com/mlresearch/v267/main/assets/elmoznino25a/elmoznino25a.pdf},
	abstract     = {Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought and language. In AI, it enables a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it. Here, we propose such a definition called representational compositionality that is conceptually simple, quantitative, and grounded in algorithmic information theory. Intuitively, representational compositionality states that a compositional representation is both expressive and describable as a simple function of parts. We validate our definition on both real and synthetic data, and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality.},
	bibtex_show  = {true}
}

@inproceedings{elmoznino2025in-context,
	title        = {In-Context Learning and Occam's Razor},
	author       = {Elmoznino, Eric and Marty, Tom and Kasetty, Tejas and Gagnon, Leo and Mittal, Sarthak and Fathi, Mahan and Sridhar, Dhanya and Lajoie, Guillaume},
	year         = 2025,
	month        = {13--19 Jul},
	booktitle    = {Proceedings of the 42nd International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 267,
	pages        = {15296--15319},
	url          = {https://proceedings.mlr.press/v267/elmoznino25b.html},
	abbr         = {ICML},
	editor       = {Singh, Aarti and Fazel, Maryam and Hsu, Daniel and Lacoste-Julien, Simon and Berkenkamp, Felix and Maharaj, Tegan and Wagstaff, Kiri and Zhu, Jerry},
	pdf          = {https://raw.githubusercontent.com/mlresearch/v267/main/assets/elmoznino25b/elmoznino25b.pdf},
	bibtex_show  = {true}
}

@inproceedings{mittal2025does,
	title        = {Does learning the right latent variables necessarily improve in-context learning?},
	author       = {Mittal, Sarthak and Elmoznino, Eric and Gagnon, Leo and Bhardwaj, Sangnie and Lajoie, Guillaume and Sridhar, Dhanya},
	year         = 2025,
	month        = {13--19 Jul},
	booktitle    = {Proceedings of the 42nd International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 267,
	pages        = {44504--44530},
	url          = {https://proceedings.mlr.press/v267/mittal25a.html},
	abbr         = {ICML},
	editor       = {Singh, Aarti and Fazel, Maryam and Hsu, Daniel and Lacoste-Julien, Simon and Berkenkamp, Felix and Maharaj, Tegan and Wagstaff, Kiri and Zhu, Jerry},
	pdf          = {https://raw.githubusercontent.com/mlresearch/v267/main/assets/mittal25a/mittal25a.pdf},
	bibtex_show  = {true}
}

@inproceedings{gagnon2025next-token,
	title        = {Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective},
	author       = {L\'{e}o Gagnon and Eric Elmoznino and Sarthak Mittal and Tom Marty and Tejas Kasetty and Dhanya Sridhar and Guillaume Lajoie},
	year         = 2025,
	booktitle    = {ICML 2025 Workshop on Efficient Systems for Foundation Models},
	url          = {https://openreview.net/forum?id=vE0bhgmDze},
	abbr         = {ICML},
	bibtex_show  = {true}
}

@article{moudgil2025learning,
	title        = {Celo: Training Versatile Learned Optimizers on a Compute Diet},
	author       = {Abhinav Moudgil and Boris Knyazev and Guillaume Lajoie and Eugene Belilovsky},
	year         = 2025,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=SLqJbt4emY},
	abbr         = {TMLR},
	bibtex_show  = {true}
}

@article{milisav2025neuromorphic,
	title        = {Neuromorphic hierarchical modular reservoirs},
	author       = {Milisav, Filip and Luppi, Andrea I. and Su\'{a}rez, Laura E. and Lajoie, Guillaume and Misic, Bratislav},
	year         = 2025,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2025.06.20.660760},
	url          = {https://www.biorxiv.org/content/early/2025/06/21/2025.06.20.660760},
	abbr         = {bioRxiv},
	elocation-id = {2025.06.20.660760},
	bibtex_show  = {true}
}

@misc{guerra2025bidirectionalinformationflowbif,
	title        = {Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization},
	author       = {Juan D. Guerra and Thomas Garbay and Guillaume Lajoie and Marco Bonizzato},
	year         = 2025,
	url          = {https://arxiv.org/abs/2505.11294},
	eprint       = {2505.11294},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{afrasiyabi2025latent,
	title        = {Latent Representation Learning for Multimodal Brain Activity Translation},
	author       = {Afrasiyabi, Arman and Bhaskar, Dhananjay and Busch, Erica L. and Caplette, Laurent and Singh, Rahul and Lajoie, Guillaume and Turk-Browne, Nicholas B. and Krishnaswamy, Smita},
	year         = 2025,
	month        = may,
	booktitle    = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1--5},
	doi          = {10.1109/icassp49660.2025.10887834},
	issn         = {2379-190x},
	abbr         = {ICASSP},
	abstract     = {Neuroscience employs diverse neuroimaging techniques, each offering distinct insights into brain activity, from electrophysiological recordings such as EEG, which have high temporal resolution, to hemodynamic modalities such as fMRI, which have increased spatial precision. However, integrating these heterogeneous data sources remains a challenge, which limits a comprehensive understanding of brain function. We present the Spatiotemporal Alignment of Multimodal Brain Activity (SAMBA) framework, which bridges the spatial and temporal resolution gaps across modalities by learning a unified latent space free of modality-specific biases. SAMBA introduces a novel attention-based wavelet decomposition for spectral filtering of electrophysiological recordings, graph attention networks to model functional connectivity between functional brain units, and recurrent layers to capture temporal autocorrelations in brain signal. We show that the training of SAMBA, aside from achieving translation, also learns a rich representation of brain information processing. We showcase this classify external stimuli driving brain activity from the representation learned in hidden layers of SAMBA, paving the way for broad downstream applications in neuroscience research and clinical contexts.},
	keywords     = {Wavelet transforms;Training;Translation;Neuroscience;Soft sensors;Transformers;Spatiotemporal phenomena;Recording;Spatial resolution;Speech processing},
	bibtex_show  = {true}
}

@inproceedings{meulemans2025multi-agent,
	title        = {Multi-agent cooperation through learning-aware policy gradients},
	author       = {Alexander Meulemans and Seijin Kobayashi and Johannes Von Oswald and Nino Scherrer and Eric Elmoznino and Blake Aaron Richards and Guillaume Lajoie and Blaise Aguera y Arcas and Joao Sacramento},
	year         = 2025,
	booktitle    = {The Thirteenth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=GkWA6NjePN},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{knyazev2025accelerating,
	title        = {Accelerating Training with Neuron Interaction and Nowcasting Networks},
	author       = {Boris Knyazev and Abhinav Moudgil and Guillaume Lajoie and Eugene Belilovsky and Simon Lacoste-Julien},
	year         = 2025,
	booktitle    = {The Thirteenth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=cUFIil6hEG},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{williams2025expressivity,
	title        = {Expressivity of Neural Networks with Random Weights and Learned Biases},
	author       = {Ezekiel Williams and Alexandre Payeur and Avery Hee-Woon Ryoo and Thomas Jiralerspong and Matthew G Perich and Luca Mazzucato and Guillaume Lajoie},
	year         = 2025,
	booktitle    = {The Thirteenth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=5xwx1Myosu},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@article{busch2025accelerated,
	title        = {Accelerated learning of a noninvasive human brain-computer interface via manifold geometry},
	author       = {Busch, Erica L. and Fincke, E. Chandra and Lajoie, Guillaume and Krishnaswamy, Smita and Turk-Browne, Nicholas B.},
	year         = 2025,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2025.03.29.646109},
	url          = {https://www.biorxiv.org/content/early/2025/04/03/2025.03.29.646109},
	abbr         = {bioRxiv},
	elocation-id = {2025.03.29.646109},
	abstract     = {Brain-computer interfaces (BCIs) promise to restore and enhance a wide range of human capabilities. However, a barrier to the adoption of BCIs is how long it can take users to learn to control them. We hypothesized that human BCI learning could be accelerated by leveraging the naturally occurring geometric structure of brain activity, or its intrinsic manifold, extracted using a data-diffusion process. We trained participants on a noninvasive BCI that allowed them to gain real-time control of an avatar in a virtual reality game by modulating functional magnetic resonance imaging (fMRI) activity in brain regions that support spatial navigation. We then perturbed the mapping between fMRI activity patterns and the movement of the avatar to test our manifold hypothesis. When the new mapping respected the intrinsic manifold, participants succeeded in regaining control of the BCI by aligning their brain activity within the manifold. When the new mapping sdid not respect the intrinsic manifold, participants could not learn to control the avatar again. These findings show that the manifold geometry of brain activity constrains human learning of a complex cognitive task in higher-order brain regions. Manifold geometry may be a critical ingredient for unlocking the potential of future human neurotechnologies.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2025/04/03/2025.03.29.646109.full.pdf},
	bibtex_show  = {true}
}

@article{rajeswaran2025assistive,
	title        = {Assistive sensory-motor perturbations influence learned neural representations},
	author       = {Rajeswaran, Pavithra and Payeur, Alexandre and Lajoie, Guillaume and Orsborn, Amy L.},
	year         = 2025,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.03.20.585972},
	url          = {https://www.biorxiv.org/content/early/2025/04/02/2024.03.20.585972},
	abbr         = {bioRxiv},
	elocation-id = {2024.03.20.585972},
	abstract     = {Task errors are used to learn and refine motor skills. We investigated how task assistance influences learned neural representations using Brain-Computer Interfaces (BCIs), which map neural activity into movement via a decoder. We analyzed motor cortex activity as monkeys practiced BCI with a decoder that adapted to improve or maintain performance over days. Over time, task-relevant information became concentrated in fewer neurons, unlike with fixed decoders. At the population level, task information also became largely confined to a few neural modes that accounted for an unexpectedly small fraction of the population variance. A neural network model suggests the adaptive decoders directly contribute to forming these more compact neural representations. Our findings show that assistive decoders manipulate error information used for long-term learning computations like credit assignment, which informs our understanding of motor learning and has implications for designing real-world BCIs.Competing Interest StatementA.L.O. is a scientific advisor for Meta Reality Labs. G.L. is a scientific advisor for BIOS Health.},
	eprint       = {https://www.biorxiv.org/content/early/2025/04/02/2024.03.20.585972.full.pdf},
	bibtex_show  = {true}
}

@misc{mittal2025in-context,
	title        = {In-Context Parametric Inference: Point or Distribution Estimators?},
	author       = {Sarthak Mittal and Yoshua Bengio and Nikolay Malkin and Guillaume Lajoie},
	year         = 2025,
	url          = {https://arxiv.org/abs/2502.11617},
	eprint       = {2502.11617},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@misc{mittal2025amortized,
	title        = {Amortized In-Context Bayesian Posterior Estimation},
	author       = {Sarthak Mittal and Niels Leif Bracher and Guillaume Lajoie and Priyank Jaini and Marcus Brubaker},
	year         = 2025,
	url          = {https://arxiv.org/abs/2502.06601},
	eprint       = {2502.06601},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{guay-hottin2025robust,
	title        = {Robust prior-biased acquisition function for human-in-the-loop Bayesian optimization},
	author       = {Rose Guay-Hottin and Lison Kardassevitch and Hugo Pham and Guillaume Lajoie and Marco Bonizzato},
	year         = 2025,
	journal      = {Knowledge-Based Systems},
	volume       = 311,
	pages        = 113039,
	doi          = {10.1016/j.knosys.2025.113039},
	issn         = {0950-7051},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950705125000863},
	abbr         = {KBS},
	keywords     = {Bayesian optimization, Domain knowledge integration, Prior-weighted acquisition function, Region of interest, Human-in-the-loop},
	abstract     = {In diverse fields of application, Bayesian Optimization (BO) has been proposed to find the optimum of black-box functions, surpassing human-driven searches. BO's appeal lies in its data efficiency, making it suitable for optimizing costly-to-evaluate functions without requiring extensive training data. While BO can perform well in closed-loop, domain experts frequently have hypotheses about which parameter combinations are more likely to yield optimal results. Hence, for BO to be truly relevant and adopted by practitioners, such prior knowledge needs to be efficiently and seamlessly integrated into the optimization framework. Some methods were recently developed to address this challenge, but they suffer from robustness issues when provided erroneous insight. Building on the idea of element-wise prior-weighted acquisition function, we propose to use a fixed-weight effective prior that distills expert user knowledge with minimal computational cost. Comprehensive investigation across diverse task conditions and prior quality levels revealed that our method, \ensuremath{\alpha}-\ensuremath{\pi}BO, surpasses Vanilla BO when provided with insights of good quality while maintaining robustness against misleading information. Moreover, unlike other methods, \ensuremath{\alpha}-\ensuremath{\pi}BO typically requires no hyperparameter tuning, largely simplifying its implementation in diverse tasks.},
	bibtex_show  = {true}
}

@article{bredenberg2025oneirogen,
	title        = {The oneirogen hypothesis: modeling the hallucinatory effects of classical psychedelics in terms of replay-dependent plasticity mechanisms},
	author       = {Bredenberg, Colin and Normandin, Fabrice and Richards, Blake and Lajoie, Guillaume},
	year         = 2025,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.09.27.615483},
	url          = {https://www.biorxiv.org/content/early/2025/01/13/2024.09.27.615483},
	abbr         = {bioRxiv},
	elocation-id = {2024.09.27.615483},
	abstract     = {Classical psychedelics induce complex visual hallucinations in humans, generating percepts that are co-herent at a low level, but which have surreal, dream-like qualities at a high level. While there are many hypotheses as to how classical psychedelics could induce these effects, there are no concrete mechanistic models that capture the variety of observed effects in humans, while remaining consistent with the known pharmacological effects of classical psychedelics on neural circuits. In this work, we propose the {\textquotedblleft}oneirogen hypothesis{\textquotedblright}, which posits that the perceptual effects of classical psychedelics are a result of their pharmacological actions inducing neural activity states that truly are more similar to dream-like states. We simulate classical psychedelics{\textquoteright} effects via manipulating neural network models trained on perceptual tasks with the Wake-Sleep algorithm. This established machine learning algorithm leverages two activity phases, a perceptual phase (wake) where sensory inputs are encoded, and a generative phase (dream) where the network internally generates activity consistent with stimulus-evoked responses. We simulate the action of psychedelics by partially shifting the model to the {\textquoteleft}Sleep{\textquoteright} state, which entails a greater influence of top-down connections, in line with the impact of psychedelics on apical dendrites. The effects resulting from this manipulation capture a number of experimentally observed phenomena including the emergence of hallucinations, increases in stimulus-conditioned variability, and large increases in synaptic plasticity. We further provide a number of testable predictions which could be used to validate or invalidate our oneirogen hypothesis.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2025/01/13/2024.09.27.615483.full.pdf},
	bibtex_show  = {true}
}

@article{codol2024brain-like,
	title        = {Brain-like neural dynamics for behavioral control develop through reinforcement learning},
	author       = {Codol, Olivier and Krishna, Nanda H. and Lajoie, Guillaume and Perich, Matthew G.},
	year         = 2024,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.10.04.616712},
	url          = {https://www.biorxiv.org/content/early/2024/10/06/2024.10.04.616712},
	abbr         = {bioRxiv},
	elocation-id = {2024.10.04.616712},
	abstract     = {During development, neural circuits are shaped continuously as we learn to control our bodies. The ultimate goal of this process is to produce neural dynamics that enable the rich repertoire of behaviors we perform with our limbs. What begins as a series of {\textquotedblleft}babbles{\textquotedblright} coalesces into skilled motor output as the brain rapidly learns to control the body. However, the nature of the teaching signal underlying this normative learning process remains elusive. Here, we test two well-established and biologically plausible theories{\textemdash}supervised learning (SL) and reinforcement learning (RL){\textemdash}that could explain how neural circuits develop the capacity for skilled movements. We trained recurrent neural networks to control a biomechanical model of a primate arm using either SL or RL and compared the resulting neural dynamics to populations of neurons recorded from the motor cortex of monkeys performing the same movements. Intriguingly, only RL-trained networks produced neural activity that matched their biological counterparts in terms of both the geometry and dynamics of population activity. We show that the similarity between RL-trained networks and biological brains depends critically on matching biomechanical properties of the limb. We then demonstrated that monkeys and RL-trained networks, but not SL-trained networks, show a strikingly similar capacity for robust short-term behavioral adaptation to a movement perturbation, indicating a fundamental and general commonality in the neural control policy. Together, our results support the hypothesis that neural dynamics for behavioral control emerge through a process akin to reinforcement learning. The resulting neural circuits offer numerous advantages for adaptable behavioral control over simpler and more efficient learning rules and expand our understanding of how developmental processes shape neural dynamics.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2024/10/06/2024.10.04.616712.full.pdf},
	bibtex_show  = {true}
}

@article{geadah2024neural,
	title        = {Neural networks with optimized single-neuron adaptation uncover biologically plausible regularization},
	author       = {Geadah, Victor AND Horoi, Stefan AND Kerg, Giancarlo AND Wolf, Guy AND Lajoie, Guillaume},
	year         = 2024,
	month        = dec,
	journal      = {PLOS Computational Biology},
	publisher    = {Public Library of Science},
	volume       = 20,
	number       = 12,
	pages        = {1--23},
	doi          = {10.1371/journal.pcbi.1012567},
	url          = {https://doi.org/10.1371/journal.pcbi.1012567},
	abbr         = {PLOS Comp Bio},
	abstract     = {Neurons in the brain have rich and adaptive input-output properties. Features such as heterogeneous f-I curves and spike frequency adaptation are known to place single neurons in optimal coding regimes when facing changing stimuli. Yet, it is still unclear how brain circuits exploit single-neuron flexibility, and how network-level requirements may have shaped such cellular function. To answer this question, a multi-scaled approach is needed where the computations of single neurons and neural circuits must be considered as a complete system. In this work, we use artificial neural networks to systematically investigate single-neuron input-output adaptive mechanisms, optimized in an end-to-end fashion. Throughout the optimization process, each neuron has the liberty to modify its nonlinear activation function parametrized to mimic f-I curves of biological neurons, either by learning an individual static function or via a learned and shared adaptation mechanism to modify activation functions in real-time during a task. We find that such adaptive networks show much-improved robustness to noise and changes in input statistics. Using tools from dynamical systems theory, we analyze the role of these emergent single-neuron properties and argue that neural diversity and adaptation play an active regularization role, enabling neural circuits to optimally propagate information across time. Finally, we outline similarities between these optimized solutions and known coding strategies found in biological neurons, such as gain scaling and fractional order differentiation/integration.},
	bibtex_show  = {true}
}

@article{cornford2024brain-like,
	title        = {Brain-like learning with exponentiated gradients},
	author       = {Cornford, Jonathan and Pogodin, Roman and Ghosh, Arna and Sheng, Kaiwen and Bicknell, Brendan A. and Codol, Olivier and Clark, Beverley A. and Lajoie, Guillaume and Richards, Blake A.},
	year         = 2024,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.10.25.620272},
	url          = {https://www.biorxiv.org/content/early/2024/10/26/2024.10.25.620272},
	abbr         = {bioRxiv},
	elocation-id = {2024.10.25.620272},
	abstract     = {Computational neuroscience relies on gradient descent (GD) for training artificial neural network (ANN) models of the brain. The advantage of GD is that it is effective at learning difficult tasks. However, it produces ANNs that are a poor phenomenological fit to biology, making them less relevant as models of the brain. Specifically, it violates Dale{\textquoteright}s law, by allowing synapses to change from excitatory to inhibitory, and leads to synaptic weights that are not log-normally distributed, contradicting experimental data. Here, starting from first principles of optimisation theory, we present an alternative learning algorithm, exponentiated gradient (EG), that respects Dale{\textquoteright}s Law and produces log-normal weights, without losing the power of learning with gradients. We also show that in biologically relevant settings EG outperforms GD, including learning from sparsely relevant signals and dealing with synaptic pruning. Altogether, our results show that EG is a superior learning algorithm for modelling the brain with ANNs.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2024/10/26/2024.10.25.620272.full.pdf},
	bibtex_show  = {true}
}

@article{sainath2024task-optimized,
	title        = {Task-Optimized Artificial Neural Networks Align with Human Brain Activity in a Visual Working Memory Task},
	author       = {Sainath, Pravish and Lajoie, Guillaume and Bellec, Pierre},
	year         = 2024,
	journal      = {PsyArXiv},
	doi          = {10.31234/osf.io/7g9ej_v1},
	url          = {http://dx.doi.org/10.31234/osf.io/7g9ej_v1},
	abbr         = {PsyArXiv},
	bibtex_show  = {true}
}

@misc{kobayashi2024when,
	title        = {When can transformers compositionally generalize in-context?},
	author       = {Seijin Kobayashi and Simon Schug and Yassir Akram and Florian Redhardt and Johannes von Oswald and Razvan Pascanu and Guillaume Lajoie and Jo\~{a}o Sacramento},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.12275},
	eprint       = {2407.12275},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{paugam2024benchmark,
	title        = {A benchmark of individual auto-regressive models in a massive fMRI dataset},
	author       = {Paugam, Fran\c{c}ois and Pinsard, Basile and Lajoie, Guillaume and Bellec, Pierre},
	year         = 2024,
	month        = {07},
	journal      = {Imaging Neuroscience},
	volume       = 2,
	pages        = {1--23},
	doi          = {10.1162/imag_a_00228},
	issn         = {2837-6056},
	url          = {https://doi.org/10.1162/imag\%5Fa\%5F00228},
	abbr         = {Imaging Neurosci},
	abstract     = {Dense functional magnetic resonance imaging datasets open new avenues to create auto-regressive models of brain activity. Individual idiosyncrasies are obscured by group models, but can be captured by purely individual models given sufficient amounts of training data. In this study, we compared several deep and shallow individual models on the temporal auto-regression of BOLD time-series recorded during a natural video-watching task. The best performing models were then analyzed in terms of their data requirements and scaling, subject specificity, and the space-time structure of their predicted dynamics. We found the Chebnets, a type of graph convolutional neural network, to be best suited for temporal BOLD auto-regression, closely followed by linear models. Chebnets demonstrated an increase in performance with increasing amounts of data, with no complete saturation at 9 h of training data. Good generalization to other kinds of video stimuli and to resting-state data marked the Chebnets' ability to capture intrinsic brain dynamics rather than only stimulus-specific autocorrelation patterns. Significant subject specificity was found at short prediction time lags. The Chebnets were found to capture lower frequencies at longer prediction time lags, and the spatial correlations in predicted dynamics were found to match traditional functional connectivity networks. Overall, these results demonstrate that large individual functional magnetic resonance imaging (fMRI) datasets can be used to efficiently train purely individual auto-regressive models of brain activity, and that massive amounts of individual data are required to do so. The excellent performance of the Chebnets likely reflects their ability to combine spatial and temporal interactions on large time scales at a low complexity cost. The non-linearities of the models did not appear as a key advantage. In fact, surprisingly, linear versions of the Chebnets appeared to outperform the original non-linear ones. Individual temporal auto-regressive models have the potential to improve the predictability of the BOLD signal. This study is based on a massive, publicly-available dataset, which can serve for future benchmarks of individual auto-regressive modeling.},
	eprint       = {https://direct.mit.edu/imag/article-pdf/doi/10.1162/imag\_a\_00228/2461525/imag\_a\_00228.pdf},
	bibtex_show  = {true}
}

@article{berthon2024using,
	title        = {Using neural biomarkers to personalize dosing of vagus nerve stimulation},
	author       = {Berthon,  Antonin and Wernisch,  Lorenz and Stoukidi,  Myrta and Thornton,  Michael and Tessier-Lariviere,  Olivier and Fortier-Poisson,  Pascal and Mamen,  Jorin and Pinkney,  Max and Lee,  Susannah and Sarkans,  Elvijs and Annecchino,  Luca and Appleton,  Ben and Garsed,  Philip and Patterson,  Bret and Gonshaw,  Samuel and Jakopec,  Matjaz and Shunmugam,  Sudhakaran and Edwards,  Tristan and Tukiainen,  Aleksi and Jennings,  Joel and Lajoie,  Guillaume and Hewage,  Emil and Armitage,  Oliver},
	year         = 2024,
	month        = jun,
	journal      = {Bioelectronic Medicine},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 10,
	number       = 1,
	doi          = {10.1186/s42234-024-00147-4},
	issn         = {2332-8886},
	url          = {http://dx.doi.org/10.1186/s42234-024-00147-4},
	abbr         = {BEME},
	bibtex_show  = {true}
}

@inproceedings{jhu2024amortizing,
	title        = {Amortizing intractable inference in large language models},
	author       = {Edward J Hu and Moksh Jain and Eric Elmoznino and Younesse Kaddar and Guillaume Lajoie and Yoshua Bengio and Nikolay Malkin},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Ouj6p4ca60},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{pogodin2024synaptic,
	title        = {Synaptic Weight Distributions Depend on the Geometry of Plasticity},
	author       = {Roman Pogodin and Jonathan Cornford and Arna Ghosh and Gauthier Gidel and Guillaume Lajoie and Blake Aaron Richards},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=x5txICnnjC},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{li2024leveraging,
	title        = {Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency},
	author       = {Tianhong Li and Sangnie Bhardwaj and Yonglong Tian and Han Zhang and Jarred Barber and Dina Katabi and Guillaume Lajoie and Huiwen Chang and Dilip Krishnan},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=kNjrhD67LP},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{krishna2024sufficient,
	title        = {Sufficient conditions for offline reactivation in recurrent neural networks},
	author       = {Nanda H Krishna and Colin Bredenberg and Daniel Levenstein and Blake Aaron Richards and Guillaume Lajoie},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=RVrINT6MT7},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{renfalet2024delta-ai,
	title        = {Delta-{AI}: Local objectives for amortized inference in sparse graphical models},
	author       = {Jean-Pierre Ren{\'e} Falet and Hae Beom Lee and Nikolay Malkin and Chen Sun and Dragos Secrieru and Dinghuai Zhang and Guillaume Lajoie and Yoshua Bengio},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=LemSSn8htt},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{helenaliu2024how,
	title        = {How connectivity structure shapes rich and lazy learning in neural circuits},
	author       = {Yuhan Helena Liu and Aristide Baratin and Jonathan Cornford and Stefan Mihalas and Eric Todd SheaBrown and Guillaume Lajoie},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=slSmYGc8ee},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{white2024learning,
	title        = {Learning and Aligning Structured Random Feature Networks},
	author       = {Vivian White and Muawiz Sajjad Chaudhary and Guy Wolf and Guillaume Lajoie and Kameron Decker Harris},
	year         = 2024,
	booktitle    = {ICLR 2024 Workshop on Representational Alignment},
	url          = {https://openreview.net/forum?id=vWhUQXQoFF},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@article{wernisch2024online,
	title        = {Online Bayesian optimization of vagus nerve stimulation},
	author       = {Wernisch, Lorenz and Edwards, Tristan and Berthon, Antonin and Tessier-Lariviere, Olivier and Sarkans, Elvijs and Stoukidi, Myrta and Fortier-Poisson, Pascal and Pinkney, Max and Thornton, Michael and Hanley, Catherine and Lee, Susannah and Jennings, Joel and Appleton, Ben and Garsed, Phillip and Patterson, Bret and Buttinger, Will and Gonshaw, Samuel and Jakopec, Matja\v{z} and Shunmugam, Sudhakaran and Mamen, Jorin and Tukiainen, Aleksi and Lajoie, Guillaume and Armitage, Oliver and Hewage, Emil},
	year         = 2024,
	month        = apr,
	journal      = {Journal of Neural Engineering},
	publisher    = {IOP Publishing},
	volume       = 21,
	number       = 2,
	pages        = {026019},
	doi          = {10.1088/1741-2552/ad33ae},
	url          = {https://dx.doi.org/10.1088/1741-2552/ad33ae},
	abbr         = {JNE},
	abstract     = {Objective. In bioelectronic medicine, neuromodulation therapies induce neural signals to the brain or organs, modifying their function. Stimulation devices capable of triggering exogenous neural signals using electrical waveforms require a complex and multi-dimensional parameter space to control such waveforms. Determining the best combination of parameters (waveform optimization or dosing) for treating a particular patient's illness is therefore challenging. Comprehensive parameter searching for an optimal stimulation effect is often infeasible in a clinical setting due to the size of the parameter space. Restricting this space, however, may lead to suboptimal therapeutic results, reduced responder rates, and adverse effects. Approach. As an alternative to a full parameter search, we present a flexible machine learning, data acquisition, and processing framework for optimizing neural stimulation parameters, requiring as few steps as possible using Bayesian optimization. This optimization builds a model of the neural and physiological responses to stimulations, enabling it to optimize stimulation parameters and provide estimates of the accuracy of the response model. The vagus nerve (VN) innervates, among other thoracic and visceral organs, the heart, thus controlling heart rate (HR), making it an ideal candidate for demonstrating the effectiveness of our approach. Main results. The efficacy of our optimization approach was first evaluated on simulated neural responses, then applied to VN stimulation intraoperatively in porcine subjects. Optimization converged quickly on parameters achieving target HRs and optimizing neural B-fiber activations despite high intersubject variability. Significance. An optimized stimulation waveform was achieved in real time with far fewer stimulations than required by alternative optimization strategies, thus minimizing exposure to side effects. Uncertainty estimates helped avoiding stimulations outside a safe range. Our approach shows that a complex set of neural stimulation parameters can be optimized in real-time for a patient to achieve a personalized precision dosing.},
	bibtex_show  = {true}
}

@article{choiniere2024gaussian,
	title        = {Gaussian-process-based Bayesian optimization for neurostimulation interventions in rats},
	author       = {Choini\`{e}re, L\'{e}o and Guay-Hottin, Rose and Picard, R\'{e}mi and Lajoie, Guillaume and Bonizzato, Marco and Dancause, Numa},
	year         = 2024,
	month        = mar,
	journal      = {STAR Protocols},
	volume       = 5,
	number       = 1,
	pages        = 102885,
	doi          = {10.1016/j.xpro.2024.102885},
	issn         = {2666-1667},
	url          = {https://doi.org/10.1016/j.xpro.2024.102885},
	abbr         = {STAR Protocols},
	bibtex_show  = {true}
}

@article{mao2024personalized,
	title        = {Personalized inference for neurostimulation with meta-learning: a case study of vagus nerve stimulation},
	author       = {Mao, Ximeng and Chang, Yao-Chuan and Zanos, Stavros and Lajoie, Guillaume},
	year         = 2024,
	month        = jan,
	journal      = {Journal of Neural Engineering},
	publisher    = {IOP Publishing},
	volume       = 21,
	number       = 1,
	pages        = {016004},
	doi          = {10.1088/1741-2552/ad17f4},
	url          = {https://dx.doi.org/10.1088/1741-2552/ad17f4},
	abbr         = {JNE},
	abstract     = {Objective. Neurostimulation is emerging as treatment for several diseases of the brain and peripheral organs. Due to variability arising from placement of stimulation devices, underlying neuroanatomy and physiological responses to stimulation, it is essential that neurostimulation protocols are personalized to maximize efficacy and safety. Building such personalized protocols would benefit from accumulated information in increasingly large datasets of other individuals' responses. Approach. To address that need, we propose a meta-learning family of algorithms to conduct few-shot optimization of key fitting parameters of physiological and neural responses in new individuals. While our method is agnostic to neurostimulation setting, here we demonstrate its effectiveness on the problem of physiological modeling of fiber recruitment during vagus nerve stimulation (VNS). Using data from acute VNS experiments, the mapping between amplitudes of stimulus-evoked compound action potentials (eCAPs) and physiological responses, such as heart rate and breathing interval modulation, is inferred. Main results. Using additional synthetic data sets to complement experimental results, we demonstrate that our meta-learning framework is capable of directly modeling the physiology-eCAP relationship for individual subjects with much fewer individually queried data points than standard methods. Significance. Our meta-learning framework is general and can be adapted to many input–response neurostimulation mapping problems. Moreover, this method leverages information from growing data sets of past patients, as a treatment is deployed. It can also be combined with several model types, including regression, Gaussian processes with Bayesian optimization, and beyond.},
	bibtex_show  = {true}
}

@article{suárez2024connectome-based,
	title        = {Connectome-based reservoir computing with the conn2res toolbox},
	author       = {Su\'{a}rez,  Laura E. and Mihalik,  Agoston and Milisav,  Filip and Marshall,  Kenji and Li,  Mingze and V\'{e}rtes,  Petra E. and Lajoie,  Guillaume and Misic,  Bratislav},
	year         = 2024,
	month        = jan,
	journal      = {Nature Communications},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 15,
	number       = 1,
	doi          = {10.1038/s41467-024-44900-4},
	issn         = {2041-1723},
	url          = {http://dx.doi.org/10.1038/s41467-024-44900-4},
	abbr         = {Nat Commun},
	bibtex_show  = {true}
}

@misc{nam2024discrete,
	title        = {Discrete, compositional, and symbolic representations through attractor dynamics},
	author       = {Andrew Nam and Eric Elmoznino and Nikolay Malkin and James McClelland and Yoshua Bengio and Guillaume Lajoie},
	year         = 2024,
	url          = {https://arxiv.org/abs/2310.01807},
	eprint       = {2310.01807},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{ji2024sources,
	title        = {Sources of richness and ineffability for phenomenally conscious states},
	author       = {Ji, Xu and Elmoznino, Eric and Deane, George and Constant, Axel and Dumas, Guillaume and Lajoie, Guillaume and Simon, Jonathan and Bengio, Yoshua},
	year         = 2024,
	month        = {03},
	journal      = {Neuroscience of Consciousness},
	volume       = 2024,
	number       = 1,
	pages        = {niae001},
	doi          = {10.1093/nc/niae001},
	issn         = {2057-2107},
	url          = {https://doi.org/10.1093/nc/niae001},
	abbr         = {NC},
	abstract     = {Conscious states--state that there is something it is like to be in--seem both rich or full of detail and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function of two individuals relates to improved communicability of their experiences to each other. While our model may not settle all questions relating to the explanatory gap, it makes progress toward a fully physicalist explanation of the richness and ineffability of conscious experience--two important aspects that seem to be part of what makes qualitative character so puzzling.},
	eprint       = {https://academic.oup.com/nc/article-pdf/2024/1/niae001/60791056/niae001.pdf},
	bibtex_show  = {true}
}

@inproceedings{bredenberg2023formalizing,
	title        = {Formalizing locality for normative synaptic plasticity models},
	author       = {Bredenberg, Colin and Williams, Ezekiel and Savin, Cristina and Richards, Blake and Lajoie, Guillaume},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {5653--5684},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2023/file/120339238f293d4ae53a7167403abc4b-Paper-Conference.pdf},
	abbr         = {NeurIPS},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
	bibtex_show  = {true}
}

@inproceedings{azabou2023unified,
	title        = {A Unified, Scalable Framework for Neural Population Decoding},
	author       = {Azabou, Mehdi and Arora, Vinam and Ganesh, Venkataramana and Mao, Ximeng and Nachimuthu, Santosh and Mendelson, Michael and Richards, Blake and Perich, Matthew and Lajoie, Guillaume and Dyer, Eva},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {44937--44956},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2023/file/8ca113d122584f12a6727341aaf58887-Paper-Conference.pdf},
	abbr         = {NeurIPS},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
	bibtex_show  = {true}
}

@inproceedings{williams2023flexible,
	title        = {Flexible Phase Dynamics for Bio-Plausible Contrastive Learning},
	author       = {Williams, Ezekiel and Bredenberg, Colin and Lajoie, Guillaume},
	year         = 2023,
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 202,
	pages        = {37042--37065},
	url          = {https://proceedings.mlr.press/v202/williams23a.html},
	abbr         = {ICML},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/williams23a/williams23a.pdf},
	abstract     = {Many learning algorithms used as normative models in neuroscience or as candidate approaches for learning on neuromorphic chips learn by contrasting one set of network states with another. These Contrastive Learning (CL) algorithms are traditionally implemented with rigid, temporally non-local, and periodic learning dynamics, that could limit the range of physical systems capable of harnessing CL. In this study, we build on recent work exploring how CL might be implemented by biological or neurmorphic systems and show that this form of learning can be made temporally local, and can still function even if many of the dynamical requirements of standard training procedures are relaxed. Thanks to a set of general theorems corroborated by numerical experiments across several CL models, our results provide theoretical foundations for the study and development of CL methods for biological and neuromorphic neural networks.},
	bibtex_show  = {true}
}

@article{bonizzato2023autonomous,
	title        = {Autonomous optimization of neuroprosthetic stimulation parameters that drive the motor cortex and spinal cord outputs in rats and monkeys},
	author       = {Bonizzato,  Marco and Guay Hottin,  Rose and C\^ot\'{e},  Sandrine L. and Massai,  Elena and Choini\`{e}re,  L\'{e}o and Macar,  Uzay and Laferri\`{e}re,  Samuel and Sirpal,  Parikshat and Quessy,  Stephan and Lajoie,  Guillaume and Martinez,  Marina and Dancause,  Numa},
	year         = 2023,
	month        = apr,
	journal      = {Cell Reports Medicine},
	publisher    = {Elsevier BV},
	volume       = 4,
	number       = 4,
	pages        = 101008,
	doi          = {10.1016/j.xcrm.2023.101008},
	issn         = {2666-3791},
	url          = {http://dx.doi.org/10.1016/j.xcrm.2023.101008},
	abbr         = {Cell Rep Med},
	bibtex_show  = {true}
}

@article{bergeron2023use,
	title        = {Use of Invasive Brain-Computer Interfaces in Pediatric Neurosurgery: Technical and Ethical Considerations},
	author       = {David Bergeron and Christian Iorio-Morin and Marco Bonizzato and Guillaume Lajoie and Nathalie Orr Gaucher and \'{E}ric Racine and Alexander G. Weil},
	year         = 2023,
	journal      = {Journal of Child Neurology},
	volume       = 38,
	number       = {3-4},
	pages        = {223--238},
	doi          = {10.1177/08830738231167736},
	url          = {https://doi.org/10.1177/08830738231167736},
	note         = {Pmid: 37116888},
	abbr         = {J Child Neurol},
	eprint       = {https://doi.org/10.1177/08830738231167736},
	abstract     = {Invasive brain-computer interfaces hold promise to alleviate disabilities in individuals with neurologic injury, with fully implantable brain-computer interface systems expected to reach the clinic in the upcoming decade. Children with severe neurologic disabilities, like quadriplegic cerebral palsy or cervical spine trauma, could benefit from this technology. However, they have been excluded from clinical trials of intracortical brain-computer interface to date. In this manuscript, we discuss the ethical considerations related to the use of invasive brain-computer interface in children with severe neurologic disabilities. We first review the technical hardware and software considerations for the application of intracortical brain-computer interface in children. We then discuss ethical issues related to motor brain-computer interface use in pediatric neurosurgery. Finally, based on the input of a multidisciplinary panel of experts in fields related to brain-computer interface (functional and restorative neurosurgery, pediatric neurosurgery, mathematics and artificial intelligence research, neuroengineering, pediatric ethics, and pragmatic ethics), we then formulate initial recommendations regarding the clinical use of invasive brain-computer interfaces in children.},
	bibtex_show  = {true}
}

@article{busch2023multi-view,
	title        = {Multi-view manifold learning of human brain-state trajectories},
	author       = {Busch,  Erica L. and Huang,  Jessie and Benz,  Andrew and Wallenstein,  Tom and Lajoie,  Guillaume and Wolf,  Guy and Krishnaswamy,  Smita and Turk-Browne,  Nicholas B.},
	year         = 2023,
	month        = mar,
	journal      = {Nature Computational Science},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 3,
	number       = 3,
	pages        = {240–253},
	doi          = {10.1038/s43588-023-00419-0},
	issn         = {2662-8457},
	url          = {http://dx.doi.org/10.1038/s43588-023-00419-0},
	abbr         = {Nat Comp Sci},
	bibtex_show  = {true}
}

@misc{bhardwaj2023steerable,
	title        = {Steerable Equivariant Representation Learning},
	author       = {Sangnie Bhardwaj and Willie McClinton and Tongzhou Wang and Guillaume Lajoie and Chen Sun and Phillip Isola and Dilip Krishnan},
	year         = 2023,
	url          = {https://arxiv.org/abs/2302.11349},
	eprint       = {2302.11349},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{moudgil2023learning,
	title        = {Learning to Optimize with Recurrent Hierarchical Transformers},
	author       = {Abhinav Moudgil and Boris Knyazev and Guillaume Lajoie and Eugene Belilovsky},
	year         = 2023,
	booktitle    = {ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems},
	url          = {https://openreview.net/forum?id=MusMaHCrs2},
	abbr         = {ICML},
	bibtex_show  = {true}
}

@article{payeur2023neural,
	title        = {Neural manifolds and learning regimes in neural-interface tasks},
	author       = {Payeur, Alexandre and Orsborn, Amy L. and Lajoie, Guillaume},
	year         = 2023,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2023.03.11.532146},
	url          = {https://www.biorxiv.org/content/early/2023/12/23/2023.03.11.532146},
	abbr         = {bioRxiv},
	elocation-id = {2023.03.11.532146},
	abstract     = {Neural activity tends to reside on manifolds whose dimension is lower than the dimension of the whole neural state space. Experiments using brain-computer interfaces (BCIs) with microelectrode arrays implanted in the motor cortex of nonhuman primates have provided ways to test whether neural manifolds influence learning-related neural computations. Starting from a learned BCI-controlled motor task, these experiments explored the effect of changing the BCI decoder to implement perturbations that were either {\textquotedblleft}aligned{\textquotedblright} or not with the pre-existing neural manifold. In a series of studies, researchers found that within-manifold perturbations (WMPs) evoked fast reassociations of existing neural patterns for rapid adaptation, while outside-manifold perturbations (OMPs) triggered a slower adaptation process that led to the emergence of new neural patterns. Together, these findings have been interpreted as suggesting that these different rates of adaptation might be associated with distinct learning mechanisms. Here, we investigated whether gradient-descent learning could alone explain these differences. Using an idealized model that captures the fixed-point dynamics of recurrent neural networks, we uncovered gradient-based learning dynamics consistent with experimental findings. Crucially, this experimental match arose only when the network was initialized in a lazier learning regime, a concept inherited from deep learning theory. A lazy learning regime{\textemdash}in contrast with a rich regime{\textemdash}implies small changes on synaptic strengths throughout learning. For OMPs, these small changes were less effective at increasing performance and could lead to unstable adaptation with a heightened sensitivity to learning rates. For WMPs, they helped reproduce the reassociation mechanism on short adaptation time scales, especially with large input variances. Since gradient descent has many biologically plausible variants, our findings establish lazy gradient-based learning as a plausible mechanism for adaptation under network-level constraints and unify several experimental results from the literature.Competing Interest StatementALO is a scientific advisor for Meta Reality Labs. GL is a scientific advisor for BIOS Health.},
	eprint       = {https://www.biorxiv.org/content/early/2023/12/23/2023.03.11.532146.full.pdf},
	bibtex_show  = {true}
}

@article{askarihemmat2023lead,
	title        = {{LEAD}: Min-Max Optimization from a Physical Perspective},
	author       = {Reyhane Askari Hemmat and Amartya Mitra and Guillaume Lajoie and Ioannis Mitliagkas},
	year         = 2023,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=vXSsTYs6ZB},
	note         = {Featured Certification},
	abbr         = {TMLR},
	bibtex_show  = {true}
}

@inproceedings{ghosh2023how,
	title        = {How gradient estimator variance and bias impact learning in neural networks},
	author       = {Arna Ghosh and Yuhan Helena Liu and Guillaume Lajoie and Konrad Kording and Blake Aaron Richards},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=EBC60mxBwyw},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@inproceedings{davari2023reliability,
	title        = {Reliability of {CKA} as a Similarity Measure in Deep Learning},
	author       = {MohammadReza Davari and Stefan Horoi and Amine Natik and Guillaume Lajoie and Guy Wolf and Eugene Belilovsky},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=8HRvyxc606},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@article{kalajdzievski2023transfer,
	title        = {Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer},
	author       = {Damjan Kalajdzievski and Ximeng Mao and Pascal Fortier-Poisson and Guillaume Lajoie and Blake Aaron Richards},
	year         = 2023,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=kJcwlP7BRs},
	abbr         = {TMLR},
	bibtex_show  = {true}
}

@inproceedings{mittal2022is,
	title        = {Is a Modular Architecture Enough?},
	author       = {Mittal, Sarthak and Bengio, Yoshua and Lajoie, Guillaume},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 35,
	pages        = {28747--28760},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/file/b8d1d741f137d9b6ac4f3c1683791e4a-Paper-Conference.pdf},
	abbr         = {NeurIPS},
	editor       = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	bibtex_show  = {true}
}

@inproceedings{liu2022accuracy,
	title        = {Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules},
	author       = {Liu, Yuhan Helena and Ghosh, Arna and Richards, Blake and Shea-Brown, Eric and Lajoie, Guillaume},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 35,
	pages        = {23077--23097},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/file/9226f8122feb9c229c1efd9270ce7021-Paper-Conference.pdf},
	abbr         = {NeurIPS},
	editor       = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	bibtex_show  = {true}
}

@article{suarez2022connectomics-based,
	title        = {A connectomics-based taxonomy of mammals},
	author       = {Suarez, Laura E and Yovel, Yossi and van den Heuvel, Martijn P and Sporns, Olaf and Assaf, Yaniv and Lajoie, Guillaume and Misic, Bratislav},
	year         = 2022,
	month        = nov,
	journal      = {eLife},
	publisher    = {eLife Sciences Publications, Ltd},
	volume       = 11,
	pages        = {e78635},
	doi          = {10.7554/eLife.78635},
	issn         = {2050-084x},
	url          = {https://doi.org/10.7554/eLife.78635},
	abbr         = {eLife},
	article_type = {journal},
	editor       = {Baker, Chris I and Jbabdi, Saad and Heuer, Katja},
	pub_date     = {2022-11-07},
	citation     = {eLife 2022;11:e78635},
	abstract     = {Mammalian taxonomies are conventionally defined by morphological traits and genetics. How species differ in terms of neural circuits and whether inter-species differences in neural circuit organization conform to these taxonomies is unknown. The main obstacle to the comparison of neural architectures has been differences in network reconstruction techniques, yielding species-specific connectomes that are not directly comparable to one another. Here, we comprehensively chart connectome organization across the mammalian phylogenetic spectrum using a common reconstruction protocol. We analyse the mammalian MRI (MaMI) data set, a database that encompasses high-resolution ex vivo structural and diffusion MRI scans of 124 species across 12 taxonomic orders and 5 superorders, collected using a unified MRI protocol. We assess similarity between species connectomes using two methods: similarity of Laplacian eigenspectra and similarity of multiscale topological features. We find greater inter-species similarities among species within the same taxonomic order, suggesting that connectome organization reflects established taxonomic relationships defined by morphology and genetics. While all connectomes retain hallmark global features and relative proportions of connection classes, inter-species variation is driven by local regional connectivity profiles. By encoding connectomes into a common frame of reference, these findings establish a foundation for investigating how neural circuits change over phylogeny, forging a link from genes to circuits to behaviour.},
	keywords     = {connectomics, mammals, taxonomy},
	bibtex_show  = {true}
}

@misc{mittal2022points,
	title        = {From Points to Functions: Infinite-dimensional Representations in Diffusion Models},
	author       = {Sarthak Mittal and Guillaume Lajoie and Stefan Bauer and Arash Mehrjou},
	year         = 2022,
	url          = {https://arxiv.org/abs/2210.13774},
	eprint       = {2210.13774},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{george2022lazy,
	title        = {Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty},
	author       = {Thomas George and Guillaume Lajoie and Aristide Baratin},
	year         = 2022,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=lukVf4VrfP},
	abbr         = {TMLR},
	bibtex_show  = {true}
}

@inproceedings{huang2022learning,
	title        = {Learning Shared Neural Manifolds from Multi-Subject FMRI Data},
	author       = {Huang, Jessie and Busch, Erica and Wallenstein, Tom and Gerasimiuk, Michal and Benz, Andrew and Lajoie, Guillaume and Wolf, Guy and Turk-Browne, Nicholas and Krishnaswamy, Smita},
	year         = 2022,
	month        = aug,
	booktitle    = {2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)},
	pages        = {01--06},
	doi          = {10.1109/mlsp55214.2022.9943383},
	issn         = {2161-0371},
	abbr         = {MLSP},
	abstract     = {Functional magnetic resonance imaging (fMRI) data is collected in millions of noisy, redundant dimensions. To understand how different brains process the same stimulus, we aim to denoise the fMRI signal via a meaningful embedding space that captures the data's intrinsic structure as shared across brains. We assume that stimulus-driven responses share latent features common across subjects that are jointly discoverable. Previous approaches to this problem have relied on linear methods like principal component analysis and shared response modeling. We propose a neural network called MRMD-AE (manifold-regularized multiple- decoder, autoencoder) that learns a common embedding from multi-subject fMRI data while retaining the ability to decode individual responses. Our latent common space represents an extensible manifold (where untrained data can be mapped) and improves classification accuracy of stimulus features of unseen timepoints, as well as cross-subject translation of fMRI signals.},
	keywords     = {Manifolds;Conferences;Machine learning;Functional magnetic resonance imaging;Signal processing;Brain modeling;Decoding},
	bibtex_show  = {true}
}

@inproceedings{pezeshki2022multi-scale,
	title        = {Multi-scale Feature Learning Dynamics: Insights for Double Descent},
	author       = {Pezeshki, Mohammad and Mitra, Amartya and Bengio, Yoshua and Lajoie, Guillaume},
	year         = 2022,
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {17669--17690},
	url          = {https://proceedings.mlr.press/v162/pezeshki22a.html},
	abbr         = {ICML},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/pezeshki22a/pezeshki22a.pdf},
	abstract     = {An intriguing phenomenon that arises from the high-dimensional learning dynamics of neural networks is the phenomenon of ``double descent''. The more commonly studied aspect of this phenomenon corresponds to <em>model-wise</em> double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied <em>epoch-wise</em> double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. We study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions describing the generalization error in terms of low-dimensional scalar macroscopic variables. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical simulations where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks.},
	bibtex_show  = {true}
}

@misc{kerg2022neural,
	title        = {On Neural Architecture Inductive Biases for Relational Tasks},
	author       = {Giancarlo Kerg and Sarthak Mittal and David Rolnick and Yoshua Bengio and Blake Richards and Guillaume Lajoie},
	year         = 2022,
	url          = {https://arxiv.org/abs/2206.05056},
	eprint       = {2206.05056},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{farrell2022gradient-based,
	title        = {Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion},
	author       = {Farrell,  Matthew and Recanatesi,  Stefano and Moore,  Timothy and Lajoie,  Guillaume and Shea-Brown,  Eric},
	year         = 2022,
	month        = jun,
	journal      = {Nature Machine Intelligence},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 4,
	number       = 6,
	pages        = {564–573},
	doi          = {10.1038/s42256-022-00498-0},
	issn         = {2522-5839},
	url          = {http://dx.doi.org/10.1038/s42256-022-00498-0},
	abbr         = {Nat Mach Intell},
	bibtex_show  = {true}
}

@article{puelmatouzel2022performance-gated,
	title        = {Performance-gated deliberation: A context-adapted strategy in which urgency is opportunity cost},
	author       = {Puelma Touzel, Maximilian AND Cisek, Paul AND Lajoie, Guillaume},
	year         = 2022,
	month        = {05},
	journal      = {PLOS Computational Biology},
	publisher    = {Public Library of Science},
	volume       = 18,
	number       = 5,
	pages        = {1--33},
	doi          = {10.1371/journal.pcbi.1010080},
	url          = {https://doi.org/10.1371/journal.pcbi.1010080},
	abbr         = {PLOS Comp Bio},
	abstract     = {Finding the right amount of deliberation, between insufficient and excessive, is a hard decision making problem that depends on the value we place on our time. Average-reward, putatively encoded by tonic dopamine, serves in existing reinforcement learning theory as the opportunity cost of time, including deliberation time. Importantly, this cost can itself vary with the environmental context and is not trivial to estimate. Here, we propose how the opportunity cost of deliberation can be estimated adaptively on multiple timescales to account for non-stationary contextual factors. We use it in a simple decision-making heuristic based on average-reward reinforcement learning (AR-RL) that we call Performance-Gated Deliberation (PGD). We propose PGD as a strategy used by animals wherein deliberation cost is implemented directly as urgency, a previously characterized neural signal effectively controlling the speed of the decision-making process. We show PGD outperforms AR-RL solutions in explaining behaviour and urgency of non-human primates in a context-varying random walk prediction task and is consistent with relative performance and urgency in a context-varying random dot motion task. We make readily testable predictions for both neural activity and behaviour.},
	bibtex_show  = {true}
}

@inproceedings{tong2022embedding,
	title        = {Embedding Signals on Graphs with Unbalanced Diffusion Earth Mover's Distance},
	author       = {Tong, Alexander and Huguet, Guillaume and Shung, Dennis and Natik, Amine and Kuchroo, Manik and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2022,
	month        = may,
	booktitle    = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {5647--5651},
	doi          = {10.1109/icassp43922.2022.9746556},
	issn         = {2379-190x},
	abbr         = {ICASSP},
	abstract     = {In modern relational machine learning it is common to encounter large graphs that arise via interactions or similarities between observations in many domains. Further, in many cases the target entities for analysis are actually signals on such graphs. We propose to compare and organize such datasets of graph signals by using an earth mover's distance (EMD) with a geodesic cost over the underlying graph. Typically, EMD is computed by optimizing over the cost of transporting one probability distribution to another over an underlying metric space. However, this is inefficient when computing the EMD between many signals. Here, we propose an unbalanced graph EMD that efficiently embeds the unbalanced EMD on an underlying graph into an L1 space, whose metric we call unbalanced diffusion earth mover's distance (UDEMD). Next, we show how this gives distances between graph signals that are robust to noise. Finally, we apply this to organizing patients based on clinical notes, embedding cells modeled as signals on a gene graph, and organizing genes modeled as signals over a large cell graph. In each case, we show that UDEMD-based embeddings find accurate distances that are highly efficient compared to other methods.},
	keywords     = {Earth;Costs;Conferences;Computational modeling;Biological system modeling;Machine learning;Signal processing;Optimal transport;graph signal processing;knowledge graph;graph diffusion},
	bibtex_show  = {true}
}

@inproceedings{horoi2022exploring,
	title        = {Exploring the Geometry and Topology of Neural Network Loss Landscapes},
	author       = {Horoi, Stefan and Huang, Jessie and Rieck, Bastian and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2022,
	booktitle    = {Advances in Intelligent Data Analysis XX},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {171--184},
	isbn         = {978-3-031-01333-1},
	abbr         = {IDA},
	editor       = {Bouadi, Tassadit and Fromont, Elisa and H{\"u}llermeier, Eyke},
	abstract     = {Recent work has established clear links between the generalization performance of trained neural networks and the geometry of their loss landscape near the local minima to which they converge. This suggests that qualitative and quantitative examination of the loss landscape geometry could yield insights about neural network generalization performance during training. To this end, researchers have proposed visualizing the loss landscape through the use of simple dimensionality reduction techniques. However, such visualization methods have been limited by their linear nature and only capture features in one or two dimensions, thus restricting sampling of the loss landscape to lines or planes. Here, we expand and improve upon these in three ways. First, we present a novel ``jump and retrain'' procedure for sampling relevant portions of the loss landscape. We show that the resulting sampled data holds more meaningful information about the network's ability to generalize. Next, we show that non-linear dimensionality reduction of the jump and retrain trajectories via PHATE, a trajectory and manifold-preserving method, allows us to visualize differences between networks that are generalizing well vs poorly. Finally, we combine PHATE trajectories with a computational homology characterization to quantify trajectory differences.},
	bibtex_show  = {true}
}

@inproceedings{deleu2022continuous-time,
	title        = {Continuous-Time Meta-Learning with Forward Mode Differentiation},
	author       = {Tristan Deleu and David Kanaa and Leo Feng and Giancarlo Kerg and Yoshua Bengio and Guillaume Lajoie and Pierre-Luc Bacon},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=57PipS27Km},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@article{10.3389/fams.2022.818799,
	title        = {On Lyapunov Exponents for RNNs: Understanding Information Propagation Using Dynamical Systems Tools},
	author       = {Vogt, Ryan  and Puelma Touzel, Maximilian  and Shlizerman, Eli  and Lajoie, Guillaume},
	year         = 2022,
	journal      = {Frontiers in Applied Mathematics and Statistics},
	volume       = {Volume 8 - 2022},
	doi          = {10.3389/fams.2022.818799},
	issn         = {2297-4687},
	url          = {https://www.frontiersin.org/journals/applied-mathematics-and-statistics/articles/10.3389/fams.2022.818799},
	abbr         = {Front AMS},
	abstract     = {<p>Recurrent neural networks (RNNs) have been successfully applied to a variety of problems involving sequential data, but their optimization is sensitive to parameter initialization, architecture, and optimizer hyperparameters. Considering RNNs as dynamical systems, a natural way to capture stability, i.e., the growth and decay over long iterates, are the Lyapunov Exponents (LEs), which form the Lyapunov spectrum. The LEs have a bearing on stability of RNN training dynamics since forward propagation of information is related to the backward propagation of error gradients. LEs measure the asymptotic rates of expansion and contraction of non-linear system trajectories, and generalize stability analysis to the time-varying attractors structuring the non-autonomous dynamics of data-driven RNNs. As a tool to understand and exploit stability of training dynamics, the Lyapunov spectrum fills an existing gap between prescriptive mathematical approaches of limited scope and computationally-expensive empirical approaches. To leverage this tool, we implement an efficient way to compute LEs for RNNs during training, discuss the aspects specific to standard RNN architectures driven by typical sequential datasets, and show that the Lyapunov spectrum can serve as a robust readout of training stability across hyperparameters. With this exposition-oriented contribution, we hope to draw attention to this under-studied, but theoretically grounded tool for understanding training stability in RNNs.</p>},
	bibtex_show  = {true}
}

@misc{gagnon2022clarifying,
	title        = {Clarifying MCMC-based training of modern EBMs : Contrastive Divergence versus Maximum Likelihood},
	author       = {L\'{e}o Gagnon and Guillaume Lajoie},
	year         = 2022,
	url          = {https://arxiv.org/abs/2202.12176},
	eprint       = {2202.12176},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{mittal2022compositional,
	title        = {Compositional Attention: Disentangling Search and Retrieval},
	author       = {Sarthak Mittal and Sharath Chandra Raparthy and Irina Rish and Yoshua Bengio and Guillaume Lajoie},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=IwJPj2MBcIa},
	abbr         = {ICLR},
	bibtex_show  = {true}
}

@misc{davidmárton2022efficient,
	title        = {Efficient and robust multi-task learning in the brain with modular latent primitives},
	author       = {Christian David M\'{a}rton and L\'{e}o Gagnon and Guillaume Lajoie and Kanaka Rajan},
	year         = 2022,
	url          = {https://arxiv.org/abs/2105.14108},
	eprint       = {2105.14108},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{pezeshki2021gradient,
	title        = {Gradient Starvation: A Learning Proclivity in Neural Networks},
	author       = {Pezeshki, Mohammad and Kaba, Oumar and Bengio, Yoshua and Courville, Aaron C and Precup, Doina and Lajoie, Guillaume},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {1256--1272},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf},
	abbr         = {NeurIPS},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	bibtex_show  = {true}
}

@article{suárez2021learning,
	title        = {Learning function from structure in neuromorphic networks},
	author       = {Su\'{a}rez,  Laura E. and Richards,  Blake A. and Lajoie,  Guillaume and Misic,  Bratislav},
	year         = 2021,
	month        = aug,
	journal      = {Nature Machine Intelligence},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 3,
	number       = 9,
	pages        = {771–786},
	doi          = {10.1038/s42256-021-00376-1},
	issn         = {2522-5839},
	url          = {http://dx.doi.org/10.1038/s42256-021-00376-1},
	abbr         = {Nat Mach Intell},
	bibtex_show  = {true}
}

@article{abrevaya2021learning,
	title        = {Learning Brain Dynamics With Coupled Low-Dimensional Nonlinear Oscillators and Deep Recurrent Networks},
	author       = {Abrevaya, Germ\'{a}n and Dumas, Guillaume and Aravkin, Aleksandr Y. and Zheng, Peng and Gagnon-Audet, Jean-Christophe and Kozloski, James and Polosecki, Pablo and Lajoie, Guillaume and Cox, David and Dawson, Silvina Ponce and Cecchi, Guillermo and Rish, Irina},
	year         = 2021,
	month        = {07},
	journal      = {Neural Computation},
	volume       = 33,
	number       = 8,
	pages        = {2087--2127},
	doi          = {10.1162/neco_a_01401},
	issn         = {0899-7667},
	url          = {https://doi.org/10.1162/neco\%5Fa\%5F01401},
	abbr         = {Neural Comput},
	abstract     = {Many natural systems, especially biological ones, exhibit complex multivariate nonlinear dynamical behaviors that can be hard to capture by linear autoregressive models. On the other hand, generic nonlinear models such as deep recurrent neural networks often require large amounts of training data, not always available in domains such as brain imaging; also, they often lack interpretability. Domain knowledge about the types of dynamics typically observed in such systems, such as a certain type of dynamical systems models, could complement purely data-driven techniques by providing a good prior. In this work, we consider a class of ordinary differential equation (ODE) models known as van der Pol (VDP) oscil lators and evaluate their ability to capture a low-dimensional representation of neural activity measured by different brain imaging modalities, such as calcium imaging (CaI) and fMRI, in different living organisms: larval zebrafish, rat, and human. We develop a novel and efficient approach to the nontrivial problem of parameters estimation for a network of coupled dynamical systems from multivariate data and demonstrate that the resulting VDP models are both accurate and interpretable, as VDP's coupling matrix reveals anatomically meaningful excitatory and inhibitory interactions across different brain subsystems. VDP outperforms linear autoregressive models (VAR) in terms of both the data fit accuracy and the quality of insight provided by the coupling matrices and often tends to generalize better to unseen data when predicting future brain activity, being comparable to and sometimes better than the recurrent neural networks (LSTMs). Finally, we demonstrate that our (generative) VDP model can also serve as a data-augmentation tool leading to marked improvements in predictive accuracy of recurrent neural networks. Thus, our work contributes to both basic and applied dimensions of neuroimaging: gaining scientific insights and improving brain-based predictive models, an area of potentially high practical importance in clinical diagnosis and neurotechnology.},
	eprint       = {https://direct.mit.edu/neco/article-pdf/33/8/2087/1930932/neco\_a\_01401.pdf},
	bibtex_show  = {true}
}

@inproceedings{ke2021systematic,
	title        = {Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning},
	author       = {Ke, Nan Rosemary and Didolkar, Aniket and Mittal, Sarthak and ALIAS PARTH GOYAL, Anirudh Goyal and Lajoie, Guillaume and Bauer, Stefan and Jimenez Rezende, Danilo and Mozer, Michael and Bengio, Yoshua and Pal, Chris},
	year         = 2021,
	booktitle    = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	volume       = 1,
	url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/8f121ce07d74717e0b1f21d122e04521-Paper-round2.pdf},
	abbr         = {NeurIPS},
	editor       = {J. Vanschoren and S. Yeung},
	bibtex_show  = {true}
}

@inproceedings{tessier-larivière2021pns-gan,
	title        = {PNS-GAN: Conditional Generation of Peripheral Nerve Signals in the Wavelet Domain via Adversarial Networks},
	author       = {Tessier-Larivi\`{e}re, Olivier and Prince, Luke Y. and Fortier-Poisson, Pascal and Wernisch, Lorenz and Armitage, Oliver and Hewage, Emil and Lajoie, Guillaume and Richards, Blake A.},
	year         = 2021,
	month        = may,
	booktitle    = {2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)},
	pages        = {778--782},
	doi          = {10.1109/ner49283.2021.9441284},
	issn         = {1948-3554},
	abbr         = {NER},
	abstract     = {Simulated datasets of neural recordings are a crucial tool in neural engineering for testing the ability of decoding algorithms to recover known ground-truth. In this work, we introduce PNS-GAN, a generative adversarial network capable of producing realistic nerve recordings conditioned on physiological biomarkers. PNS-GAN operates in the wavelet domain to preserve both the timing and frequency of neural events with high resolution. PNS-GAN generates sequences of scaleograms from noise using a recurrent neural network and 2D transposed convolution layers. PNS-GAN discriminates over stacks of scaleograms with a network of 3D convolution layers. We find that our generated signal reproduces a number of characteristics of the real signal, including similarity in a canonical time-series feature-space, and contains physiologically related neural events including respiration modulation and similar distributions of afferent and efferent signalling.},
	keywords     = {Wavelet domain;Three-dimensional displays;Recurrent neural networks;Convolution;Neural engineering;Tools;Physiology},
	bibtex_show  = {true}
}

@inproceedings{baratin2021implicit,
	title        = {Implicit Regularization via Neural Feature Alignment},
	author       = {Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Devon Hjelm, R and Lajoie, Guillaume and Vincent, Pascal and Lacoste-Julien, Simon},
	year         = 2021,
	month        = {13--15 Apr},
	booktitle    = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 130,
	pages        = {2269--2277},
	url          = {https://proceedings.mlr.press/v130/baratin21a.html},
	abbr         = {AISTATS},
	editor       = {Banerjee, Arindam and Fukumizu, Kenji},
	pdf          = {http://proceedings.mlr.press/v130/baratin21a/baratin21a.pdf},
	abstract     = {We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a regularization effect induced by a dynamical alignment ofthe neural tangent features introduced by Jacot et al. (2018), along a small number of task-relevant directions. This can be interpreted as a combined mechanism of feature selection and compression. By extrapolating a new analysis of Rademacher complexity bounds for linear models, we motivate and study a heuristic complexity measure that captures this phenomenon, in terms of sequences of tangent kernel classes along optimization paths. The code for our experiments is available as https://github.com/tfjgeorge/ntk\_alignment.},
	bibtex_show  = {true}
}

@article{recanatesi2021predictive,
	title        = {Predictive learning as a network mechanism for extracting low-dimensional latent space representations},
	author       = {Recanatesi,  Stefano and Farrell,  Matthew and Lajoie,  Guillaume and Deneve,  Sophie and Rigotti,  Mattia and Shea-Brown,  Eric},
	year         = 2021,
	month        = mar,
	journal      = {Nature Communications},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 12,
	number       = 1,
	doi          = {10.1038/s41467-021-21696-1},
	issn         = {2041-1723},
	url          = {http://dx.doi.org/10.1038/s41467-021-21696-1},
	abbr         = {Nat Commun},
	bibtex_show  = {true}
}

@inproceedings{mittal2020learning,
	title        = {Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules},
	author       = {Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua},
	year         = 2020,
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 119,
	pages        = {6972--6986},
	url          = {https://proceedings.mlr.press/v119/mittal20a.html},
	abbr         = {ICML},
	editor       = {III, Hal Daum\'{e} and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/mittal20a/mittal20a.pdf},
	abstract     = {Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and the current reportable short-term memory, such as how the phrase `peanut butter and ...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.},
	bibtex_show  = {true}
}

@misc{geadah2020advantages,
	title        = {Advantages of biologically-inspired adaptive neural activation in RNNs during learning},
	author       = {Victor Geadah and Giancarlo Kerg and Stefan Horoi and Guy Wolf and Guillaume Lajoie},
	year         = 2020,
	url          = {https://arxiv.org/abs/2006.12253},
	eprint       = {2006.12253},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{laferriere2020hierarchical,
	title        = {Hierarchical Bayesian Optimization of Spatiotemporal Neurostimulations for Targeted Motor Outputs},
	author       = {Laferri\`{e}re, Samuel and Bonizzato, Marco and C\^{o}t\'{e}, Sandrine L. and Dancause, Numa and Lajoie, Guillaume},
	year         = 2020,
	month        = jun,
	journal      = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume       = 28,
	number       = 6,
	pages        = {1452--1460},
	doi          = {10.1109/tnsre.2020.2987001},
	issn         = {1558-0210},
	abbr         = {TNSRE},
	abstract     = {The development of neurostimulation techniques to evoke motor patterns is an active area of research. It serves as a crucial experimental tool to probe computation in neural circuits, and has applications in neuroprostheses used to aid recovery of motor function after stroke or injury to the nervous system. There are two important challenges when designing algorithms to unveil and control neurostimulation-to-motor correspondences, thereby linking spatiotemporal patterns of neural stimulation to muscle activation: (1) the exploration of motor maps needs to be fast and efficient (exhaustive search is to be avoided for clinical and experimental reasons) (2) online learning needs to be flexible enough to deal with noise and occasional spurious responses. We propose a stimulation search algorithm to address these issues, and demonstrate its efficacy with experiments in the motor cortex (M1) of a non-human primate model. Our solution is a novel iterative process using Bayesian Optimization via Gaussian Processes on a hierarchy of increasingly complex signal spaces. We show that our algorithm can successfully and rapidly learn correspondences between complex stimulation patterns and evoked muscle activation patterns, where standard approaches fail. Importantly, we uncover nonlinear circuit-level computations in M1 that would have been difficult to identify using conventional mapping techniques.},
	keywords     = {Electromyography;Muscles;Optimization;Electrodes;Bayes methods;Gaussian processes;Spatiotemporal phenomena;Neural engineering;machine learning algorithms;optimization methods;motor cortex (M1);neural stimulation},
	bibtex_show  = {true}
}

@misc{horoi2020internal,
	title        = {Internal representation dynamics and geometry in recurrent neural networks},
	author       = {Stefan Horoi and Guillaume Lajoie and Guy Wolf},
	year         = 2020,
	url          = {https://arxiv.org/abs/2001.03255},
	eprint       = {2001.03255},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@inproceedings{kerg2020untangling,
	title        = {Untangling tradeoffs between recurrence and self-attention in artificial neural networks},
	author       = {Kerg, Giancarlo and Kanuparthi, Bhargav and ALIAS PARTH GOYAL, Anirudh Goyal and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {19443--19454},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf},
	abbr         = {NeurIPS},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	bibtex_show  = {true}
}

@inproceedings{horoi2020low-dimensional,
	title        = {Low-Dimensional Dynamics of Encoding and Learning in Recurrent Neural Networks},
	author       = {Horoi, Stefan and Geadah, Victor and Wolf, Guy and Lajoie, Guillaume},
	year         = 2020,
	booktitle    = {Advances in Artificial Intelligence},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {276--282},
	isbn         = {978-3-030-47358-7},
	abbr         = {CCAI},
	editor       = {Goutte, Cyril and Zhu, Xiaodan},
	abstract     = {In this paper, we use dimensionality reduction techniques to study how a recurrent neural network (RNN) processes and encodes information in the context of a classification task, and we explain our findings using tools from dynamical systems theory. We observe that internal representations develop a task-relevant structure as soon as significant information is provided as input and this structure remains for some time even if we let the dynamics drift. However, the structure is only interpretable by the final classifying layer at the fixed time step for which the network was trained. We measure that throughout the training, the recurrent weights matrix is modified so that the resulting dynamical system associated with the network's neural activations evolves into a non-trivial attractor, reminiscent of neural oscillations in the brain. Our findings suggest that RNNs change their internal dynamics throughout training so that information is stored in low-dimensional cycles, rather than in high-dimensional clusters.},
	bibtex_show  = {true}
}

@misc{recanatesi2019dimensionality,
	title        = {Dimensionality compression and expansion in Deep Neural Networks},
	author       = {Stefano Recanatesi and Matthew Farrell and Madhu Advani and Timothy Moore and Guillaume Lajoie and Eric Shea-Brown},
	year         = 2019,
	url          = {https://arxiv.org/abs/1906.00443},
	eprint       = {1906.00443},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	abbr         = {arXiv},
	bibtex_show  = {true}
}

@article{bogaard2019cortical,
	title        = {Cortical network mechanisms of anodal and cathodal transcranial direct current stimulation in awake primates},
	author       = {Bogaard, Andrew R. and Lajoie, Guillaume and Boyd, Hayley and Morse, Andrew and Zanos, Stavros and Fetz, Eberhard E.},
	year         = 2019,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/516260},
	url          = {https://www.biorxiv.org/content/early/2019/07/08/516260},
	abbr         = {bioRxiv},
	elocation-id = 516260,
	abstract     = {Transcranial direct current stimulation (tDCS) is a non-invasive neuromodulation technique that is widely used to stimulate the sensorimotor cortex, and yet the mechanism by which it influences the natural activity of cortical networks is still under debate. Here, we characterize the effects of anodal and cathodal tDCS on underlying neurons in active macaque sensorimotor cortex across a range of doses. We find changes in spike rates that are sensitive to both current intensity and polarity, behavioral state, and that are cell-type specific. At high currents, effects persist after the offset of stimulation, and the spatiotemporal activity associated with motor activity of the contralateral limb, measured by dynamics of neural ensembles, are altered. These data suggest that tDCS induces reproducible and noticeable changes in cortical neuron activity and support the theory that it affects brain activity through a combination of single neuron polarization and network interactions.},
	eprint       = {https://www.biorxiv.org/content/early/2019/07/08/516260.full.pdf},
	bibtex_show  = {true}
}

@inproceedings{kerg2019non-normal,
	title        = {Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics},
	author       = {Kerg, Giancarlo and Goyette, Kyle and Puelma Touzel, Maximilian and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and Lajoie, Guillaume},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf},
	abbr         = {NeurIPS},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	bibtex_show  = {true}
}

<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Guillaume Lajoie </title> <meta name="author" content="Guillaume Lajoie"> <meta name="description" content="publications by categories in reversed chronological order, auto-generated from &lt;a href='https://scholar.google.com/citations?user={{ site.data.socials.scholar_userid }}'&gt;Google Scholar&lt;/a&gt;"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href=""> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://guillaumelajoie.com/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> Guillaume <span class="navbar-brand-bold">Lajoie</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/group/">group </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/fun/">fun </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order, auto-generated from <a href="https://scholar.google.com/citations?user=ifu_7_0AAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="meulemans2025multi-agent" class="col-sm-8"> <div class="title">Multi-agent cooperation through learning-aware policy gradients</div> <div class="author"> Alexander Meulemans, Seijin Kobayashi, Johannes Von Oswald, Nino Scherrer, Eric Elmoznino, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Blake Aaron Richards, Guillaume Lajoie, Blaise Aguera Arcas, Joao Sacramento' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=GkWA6NjePN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">meulemans2025multi-agent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-agent cooperation through learning-aware policy gradients}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Meulemans, Alexander and Kobayashi, Seijin and Oswald, Johannes Von and Scherrer, Nino and Elmoznino, Eric and Richards, Blake Aaron and Lajoie, Guillaume and y Arcas, Blaise Aguera and Sacramento, Joao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=GkWA6NjePN}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="knyazev2025accelerating" class="col-sm-8"> <div class="title">Accelerating Training with Neuron Interaction and Nowcasting Networks</div> <div class="author"> Boris Knyazev, Abhinav Moudgil, <em>Guillaume Lajoie</em>, Eugene Belilovsky, and Simon Lacoste-Julien </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=cUFIil6hEG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">knyazev2025accelerating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating Training with Neuron Interaction and Nowcasting Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Knyazev, Boris and Moudgil, Abhinav and Lajoie, Guillaume and Belilovsky, Eugene and Lacoste-Julien, Simon}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=cUFIil6hEG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="williams2025expressivity" class="col-sm-8"> <div class="title">Expressivity of Neural Networks with Random Weights and Learned Biases</div> <div class="author"> Ezekiel Williams, Alexandre Payeur, Avery Hee-Woon Ryoo, Thomas Jiralerspong, Matthew G Perich, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Luca Mazzucato, Guillaume Lajoie' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=5xwx1Myosu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">williams2025expressivity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Expressivity of Neural Networks with Random Weights and Learned Biases}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Williams, Ezekiel and Payeur, Alexandre and Ryoo, Avery Hee-Woon and Jiralerspong, Thomas and Perich, Matthew G and Mazzucato, Luca and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=5xwx1Myosu}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://signalprocessingsociety.org/events/conferences" rel="external nofollow noopener" target="_blank">ICASSP</a> </abbr> </div> <div id="afrasiyabi2025latent" class="col-sm-8"> <div class="title">Latent Representation Learning for Multimodal Brain Activity Translation</div> <div class="author"> Arman Afrasiyabi, Dhananjay Bhaskar, Erica L. Busch, Laurent Caplette, Rahul Singh, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Guillaume Lajoie, Nicholas B. Turk-Browne, Smita Krishnaswamy' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/icassp49660.2025.10887834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Neuroscience employs diverse neuroimaging techniques, each offering distinct insights into brain activity, from electrophysiological recordings such as EEG, which have high temporal resolution, to hemodynamic modalities such as fMRI, which have increased spatial precision. However, integrating these heterogeneous data sources remains a challenge, which limits a comprehensive understanding of brain function. We present the Spatiotemporal Alignment of Multimodal Brain Activity (SAMBA) framework, which bridges the spatial and temporal resolution gaps across modalities by learning a unified latent space free of modality-specific biases. SAMBA introduces a novel attention-based wavelet decomposition for spectral filtering of electrophysiological recordings, graph attention networks to model functional connectivity between functional brain units, and recurrent layers to capture temporal autocorrelations in brain signal. We show that the training of SAMBA, aside from achieving translation, also learns a rich representation of brain information processing. We showcase this classify external stimuli driving brain activity from the representation learned in hidden layers of SAMBA, paving the way for broad downstream applications in neuroscience research and clinical contexts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">afrasiyabi2025latent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Latent Representation Learning for Multimodal Brain Activity Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Afrasiyabi, Arman and Bhaskar, Dhananjay and Busch, Erica L. and Caplette, Laurent and Singh, Rahul and Lajoie, Guillaume and Turk-Browne, Nicholas B. and Krishnaswamy, Smita}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp49660.2025.10887834}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2379-190x}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Wavelet transforms;Training;Translation;Neuroscience;Soft sensors;Transformers;Spatiotemporal phenomena;Recording;Spatial resolution;Speech processing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="mittal2025in-context" class="col-sm-8"> <div class="title">In-Context Parametric Inference: Point or Distribution Estimators?</div> <div class="author"> Sarthak Mittal, Yoshua Bengio, Nikolay Malkin, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2502.11617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mittal2025in-context</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Context Parametric Inference: Point or Distribution Estimators?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Bengio, Yoshua and Malkin, Nikolay and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2502.11617}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2502.11617}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="mittal2025amortized" class="col-sm-8"> <div class="title">Amortized In-Context Bayesian Posterior Estimation</div> <div class="author"> Sarthak Mittal, Niels Leif Bracher, <em>Guillaume Lajoie</em>, Priyank Jaini, and Marcus Brubaker </div> <div class="periodical"> May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2502.06601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mittal2025amortized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Amortized In-Context Bayesian Posterior Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Bracher, Niels Leif and Lajoie, Guillaume and Jaini, Priyank and Brubaker, Marcus}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2502.06601}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2502.06601}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.journals.elsevier.com/knowledge-based-systems" rel="external nofollow noopener" target="_blank">KBS</a> </abbr> </div> <div id="guay-hottin2025robust" class="col-sm-8"> <div class="title">Robust prior-biased acquisition function for human-in-the-loop Bayesian optimization</div> <div class="author"> Rose Guay-Hottin, Lison Kardassevitch, Hugo Pham, <em>Guillaume Lajoie</em>, and Marco Bonizzato </div> <div class="periodical"> <em>Knowledge-Based Systems</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.knosys.2025.113039" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In diverse fields of application, Bayesian Optimization (BO) has been proposed to find the optimum of black-box functions, surpassing human-driven searches. BO’s appeal lies in its data efficiency, making it suitable for optimizing costly-to-evaluate functions without requiring extensive training data. While BO can perform well in closed-loop, domain experts frequently have hypotheses about which parameter combinations are more likely to yield optimal results. Hence, for BO to be truly relevant and adopted by practitioners, such prior knowledge needs to be efficiently and seamlessly integrated into the optimization framework. Some methods were recently developed to address this challenge, but they suffer from robustness issues when provided erroneous insight. Building on the idea of element-wise prior-weighted acquisition function, we propose to use a fixed-weight effective prior that distills expert user knowledge with minimal computational cost. Comprehensive investigation across diverse task conditions and prior quality levels revealed that our method, \ensuremathα-\ensuremath\piBO, surpasses Vanilla BO when provided with insights of good quality while maintaining robustness against misleading information. Moreover, unlike other methods, \ensuremathα-\ensuremath\piBO typically requires no hyperparameter tuning, largely simplifying its implementation in diverse tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guay-hottin2025robust</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust prior-biased acquisition function for human-in-the-loop Bayesian optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guay-Hottin, Rose and Kardassevitch, Lison and Pham, Hugo and Lajoie, Guillaume and Bonizzato, Marco}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Knowledge-Based Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{311}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{113039}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.knosys.2025.113039}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0950-7051}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0950705125000863}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Bayesian optimization, Domain knowledge integration, Prior-weighted acquisition function, Region of interest, Human-in-the-loop}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="moudgil2025learning" class="col-sm-8"> <div class="title">Learning Versatile Optimizers on a Compute Diet</div> <div class="author"> Abhinav Moudgil, Boris Knyazev, <em>Guillaume Lajoie</em>, and Eugene Belilovsky </div> <div class="periodical"> May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2501.12670" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">moudgil2025learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Versatile Optimizers on a Compute Diet}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moudgil, Abhinav and Knyazev, Boris and Lajoie, Guillaume and Belilovsky, Eugene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2501.12670}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2501.12670}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="busch2025accelerated" class="col-sm-8"> <div class="title">Accelerated learning of a noninvasive human brain-computer interface via manifold geometry</div> <div class="author"> Erica L. Busch, E. Chandra Fincke, <em>Guillaume Lajoie</em>, Smita Krishnaswamy, and Nicholas B. Turk-Browne </div> <div class="periodical"> <em>bioRxiv</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2025.03.29.646109" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Brain-computer interfaces (BCIs) promise to restore and enhance a wide range of human capabilities. However, a barrier to the adoption of BCIs is how long it can take users to learn to control them. We hypothesized that human BCI learning could be accelerated by leveraging the naturally occurring geometric structure of brain activity, or its intrinsic manifold, extracted using a data-diffusion process. We trained participants on a noninvasive BCI that allowed them to gain real-time control of an avatar in a virtual reality game by modulating functional magnetic resonance imaging (fMRI) activity in brain regions that support spatial navigation. We then perturbed the mapping between fMRI activity patterns and the movement of the avatar to test our manifold hypothesis. When the new mapping respected the intrinsic manifold, participants succeeded in regaining control of the BCI by aligning their brain activity within the manifold. When the new mapping sdid not respect the intrinsic manifold, participants could not learn to control the avatar again. These findings show that the manifold geometry of brain activity constrains human learning of a complex cognitive task in higher-order brain regions. Manifold geometry may be a critical ingredient for unlocking the potential of future human neurotechnologies.Competing Interest StatementThe authors have declared no competing interest.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">busch2025accelerated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerated learning of a noninvasive human brain-computer interface via manifold geometry}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Busch, Erica L. and Fincke, E. Chandra and Lajoie, Guillaume and Krishnaswamy, Smita and Turk-Browne, Nicholas B.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2025.03.29.646109}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/04/03/2025.03.29.646109}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2025.03.29.646109}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/04/03/2025.03.29.646109.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="elmoznino2025complexity-based" class="col-sm-8"> <div class="title">A Complexity-Based Theory of Compositionality</div> <div class="author"> Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2410.14817" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">elmoznino2025complexity-based</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Complexity-Based Theory of Compositionality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Elmoznino, Eric and Jiralerspong, Thomas and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2410.14817}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.14817}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="elmoznino2025in-context" class="col-sm-8"> <div class="title">In-context learning and Occam’s razor</div> <div class="author"> Eric Elmoznino, Tom Marty, Tejas Kasetty, Leo Gagnon, Sarthak Mittal, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mahan Fathi, Dhanya Sridhar, Guillaume Lajoie' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2410.14086" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">elmoznino2025in-context</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-context learning and Occam's razor}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Elmoznino, Eric and Marty, Tom and Kasetty, Tejas and Gagnon, Leo and Mittal, Sarthak and Fathi, Mahan and Sridhar, Dhanya and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2410.14086}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.14086}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="bredenberg2025oneirogen" class="col-sm-8"> <div class="title">The oneirogen hypothesis: modeling the hallucinatory effects of classical psychedelics in terms of replay-dependent plasticity mechanisms</div> <div class="author"> Colin Bredenberg, Fabrice Normandin, Blake Richards, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>bioRxiv</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2024.09.27.615483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Classical psychedelics induce complex visual hallucinations in humans, generating percepts that are co-herent at a low level, but which have surreal, dream-like qualities at a high level. While there are many hypotheses as to how classical psychedelics could induce these effects, there are no concrete mechanistic models that capture the variety of observed effects in humans, while remaining consistent with the known pharmacological effects of classical psychedelics on neural circuits. In this work, we propose the “oneirogen hypothesis”, which posits that the perceptual effects of classical psychedelics are a result of their pharmacological actions inducing neural activity states that truly are more similar to dream-like states. We simulate classical psychedelics’ effects via manipulating neural network models trained on perceptual tasks with the Wake-Sleep algorithm. This established machine learning algorithm leverages two activity phases, a perceptual phase (wake) where sensory inputs are encoded, and a generative phase (dream) where the network internally generates activity consistent with stimulus-evoked responses. We simulate the action of psychedelics by partially shifting the model to the ‘Sleep’ state, which entails a greater influence of top-down connections, in line with the impact of psychedelics on apical dendrites. The effects resulting from this manipulation capture a number of experimentally observed phenomena including the emergence of hallucinations, increases in stimulus-conditioned variability, and large increases in synaptic plasticity. We further provide a number of testable predictions which could be used to validate or invalidate our oneirogen hypothesis.Competing Interest StatementThe authors have declared no competing interest.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bredenberg2025oneirogen</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The oneirogen hypothesis: modeling the hallucinatory effects of classical psychedelics in terms of replay-dependent plasticity mechanisms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bredenberg, Colin and Normandin, Fabrice and Richards, Blake and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2024.09.27.615483}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/01/13/2024.09.27.615483}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2024.09.27.615483}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/01/13/2024.09.27.615483.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="rajeswaran2025assistive" class="col-sm-8"> <div class="title">Assistive sensory-motor perturbations influence learned neural representations</div> <div class="author"> Pavithra Rajeswaran, Alexandre Payeur, <em>Guillaume Lajoie</em>, and Amy L. Orsborn </div> <div class="periodical"> <em>bioRxiv</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2024.03.20.585972" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Task errors are used to learn and refine motor skills. We investigated how task assistance influences learned neural representations using Brain-Computer Interfaces (BCIs), which map neural activity into movement via a decoder. We analyzed motor cortex activity as monkeys practiced BCI with a decoder that adapted to improve or maintain performance over days. Over time, task-relevant information became concentrated in fewer neurons, unlike with fixed decoders. At the population level, task information also became largely confined to a few neural modes that accounted for an unexpectedly small fraction of the population variance. A neural network model suggests the adaptive decoders directly contribute to forming these more compact neural representations. Our findings show that assistive decoders manipulate error information used for long-term learning computations like credit assignment, which informs our understanding of motor learning and has implications for designing real-world BCIs.Competing Interest StatementA.L.O. is a scientific advisor for Meta Reality Labs. G.L. is a scientific advisor for BIOS Health.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rajeswaran2025assistive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Assistive sensory-motor perturbations influence learned neural representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rajeswaran, Pavithra and Payeur, Alexandre and Lajoie, Guillaume and Orsborn, Amy L.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2024.03.20.585972}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/04/02/2024.03.20.585972}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2024.03.20.585972}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2025/04/02/2024.03.20.585972.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="codol2024brain-like" class="col-sm-8"> <div class="title">Brain-like neural dynamics for behavioral control develop through reinforcement learning</div> <div class="author"> Olivier Codol, Nanda H. Krishna, <em>Guillaume Lajoie</em>, and Matthew G. Perich </div> <div class="periodical"> <em>bioRxiv</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2024.10.04.616712" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>During development, neural circuits are shaped continuously as we learn to control our bodies. The ultimate goal of this process is to produce neural dynamics that enable the rich repertoire of behaviors we perform with our limbs. What begins as a series of “babbles” coalesces into skilled motor output as the brain rapidly learns to control the body. However, the nature of the teaching signal underlying this normative learning process remains elusive. Here, we test two well-established and biologically plausible theories—supervised learning (SL) and reinforcement learning (RL)—that could explain how neural circuits develop the capacity for skilled movements. We trained recurrent neural networks to control a biomechanical model of a primate arm using either SL or RL and compared the resulting neural dynamics to populations of neurons recorded from the motor cortex of monkeys performing the same movements. Intriguingly, only RL-trained networks produced neural activity that matched their biological counterparts in terms of both the geometry and dynamics of population activity. We show that the similarity between RL-trained networks and biological brains depends critically on matching biomechanical properties of the limb. We then demonstrated that monkeys and RL-trained networks, but not SL-trained networks, show a strikingly similar capacity for robust short-term behavioral adaptation to a movement perturbation, indicating a fundamental and general commonality in the neural control policy. Together, our results support the hypothesis that neural dynamics for behavioral control emerge through a process akin to reinforcement learning. The resulting neural circuits offer numerous advantages for adaptable behavioral control over simpler and more efficient learning rules and expand our understanding of how developmental processes shape neural dynamics.Competing Interest StatementThe authors have declared no competing interest.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">codol2024brain-like</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Brain-like neural dynamics for behavioral control develop through reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Codol, Olivier and Krishna, Nanda H. and Lajoie, Guillaume and Perich, Matthew G.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2024.10.04.616712}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2024/10/06/2024.10.04.616712}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2024.10.04.616712}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2024/10/06/2024.10.04.616712.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://journals.plos.org/ploscompbiol/" rel="external nofollow noopener" target="_blank">PLOS Comp Bio</a> </abbr> </div> <div id="geadah2024neural" class="col-sm-8"> <div class="title">Neural networks with optimized single-neuron adaptation uncover biologically plausible regularization</div> <div class="author"> Victor Geadah, Stefan Horoi, Giancarlo Kerg, Guy Wolf, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>PLOS Computational Biology</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1371/journal.pcbi.1012567" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Neurons in the brain have rich and adaptive input-output properties. Features such as heterogeneous f-I curves and spike frequency adaptation are known to place single neurons in optimal coding regimes when facing changing stimuli. Yet, it is still unclear how brain circuits exploit single-neuron flexibility, and how network-level requirements may have shaped such cellular function. To answer this question, a multi-scaled approach is needed where the computations of single neurons and neural circuits must be considered as a complete system. In this work, we use artificial neural networks to systematically investigate single-neuron input-output adaptive mechanisms, optimized in an end-to-end fashion. Throughout the optimization process, each neuron has the liberty to modify its nonlinear activation function parametrized to mimic f-I curves of biological neurons, either by learning an individual static function or via a learned and shared adaptation mechanism to modify activation functions in real-time during a task. We find that such adaptive networks show much-improved robustness to noise and changes in input statistics. Using tools from dynamical systems theory, we analyze the role of these emergent single-neuron properties and argue that neural diversity and adaptation play an active regularization role, enabling neural circuits to optimally propagate information across time. Finally, we outline similarities between these optimized solutions and known coding strategies found in biological neurons, such as gain scaling and fractional order differentiation/integration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">geadah2024neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural networks with optimized single-neuron adaptation uncover biologically plausible regularization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Geadah, Victor and Horoi, Stefan and Kerg, Giancarlo and Wolf, Guy and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{PLOS Computational Biology}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Public Library of Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1371/journal.pcbi.1012567}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1371/journal.pcbi.1012567}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="cornford2024brain-like" class="col-sm-8"> <div class="title">Brain-like learning with exponentiated gradients</div> <div class="author"> Jonathan Cornford, Roman Pogodin, Arna Ghosh, Kaiwen Sheng, Brendan A. Bicknell, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Olivier Codol, Beverley A. Clark, Guillaume Lajoie, Blake A. Richards' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>bioRxiv</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2024.10.25.620272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Computational neuroscience relies on gradient descent (GD) for training artificial neural network (ANN) models of the brain. The advantage of GD is that it is effective at learning difficult tasks. However, it produces ANNs that are a poor phenomenological fit to biology, making them less relevant as models of the brain. Specifically, it violates Dale’s law, by allowing synapses to change from excitatory to inhibitory, and leads to synaptic weights that are not log-normally distributed, contradicting experimental data. Here, starting from first principles of optimisation theory, we present an alternative learning algorithm, exponentiated gradient (EG), that respects Dale’s Law and produces log-normal weights, without losing the power of learning with gradients. We also show that in biologically relevant settings EG outperforms GD, including learning from sparsely relevant signals and dealing with synaptic pruning. Altogether, our results show that EG is a superior learning algorithm for modelling the brain with ANNs.Competing Interest StatementThe authors have declared no competing interest.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cornford2024brain-like</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Brain-like learning with exponentiated gradients}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cornford, Jonathan and Pogodin, Roman and Ghosh, Arna and Sheng, Kaiwen and Bicknell, Brendan A. and Codol, Olivier and Clark, Beverley A. and Lajoie, Guillaume and Richards, Blake A.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2024.10.25.620272}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2024/10/26/2024.10.25.620272}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2024.10.25.620272}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2024/10/26/2024.10.25.620272.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://osf.io/preprints/psyarxiv" rel="external nofollow noopener" target="_blank">PsyArXiv</a> </abbr> </div> <div id="sainath2024task-optimized" class="col-sm-8"> <div class="title">Task-Optimized Artificial Neural Networks Align with Human Brain Activity in a Visual Working Memory Task</div> <div class="author"> Pravish Sainath, <em>Guillaume Lajoie</em>, and Pierre Bellec </div> <div class="periodical"> <em>PsyArXiv</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.31234/osf.io/7g9ej_v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sainath2024task-optimized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task-Optimized Artificial Neural Networks Align with Human Brain Activity in a Visual Working Memory Task}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sainath, Pravish and Lajoie, Guillaume and Bellec, Pierre}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{PsyArXiv}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.31234/osf.io/7g9ej_v1}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.31234/osf.io/7g9ej_v1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="kobayashi2024when" class="col-sm-8"> <div class="title">When can transformers compositionally generalize in-context?</div> <div class="author"> Seijin Kobayashi, Simon Schug, Yassir Akram, Florian Redhardt, Johannes Oswald, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Razvan Pascanu, Guillaume Lajoie, João Sacramento' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2407.12275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">kobayashi2024when</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{When can transformers compositionally generalize in-context?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kobayashi, Seijin and Schug, Simon and Akram, Yassir and Redhardt, Florian and von Oswald, Johannes and Pascanu, Razvan and Lajoie, Guillaume and Sacramento, Jo\~{a}o}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2407.12275}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2407.12275}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://direct.mit.edu/imag" rel="external nofollow noopener" target="_blank">Imaging Neurosci</a> </abbr> </div> <div id="paugam2024benchmark" class="col-sm-8"> <div class="title">A benchmark of individual auto-regressive models in a massive fMRI dataset</div> <div class="author"> François Paugam, Basile Pinsard, <em>Guillaume Lajoie</em>, and Pierre Bellec </div> <div class="periodical"> <em>Imaging Neuroscience</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/imag_a_00228" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Dense functional magnetic resonance imaging datasets open new avenues to create auto-regressive models of brain activity. Individual idiosyncrasies are obscured by group models, but can be captured by purely individual models given sufficient amounts of training data. In this study, we compared several deep and shallow individual models on the temporal auto-regression of BOLD time-series recorded during a natural video-watching task. The best performing models were then analyzed in terms of their data requirements and scaling, subject specificity, and the space-time structure of their predicted dynamics. We found the Chebnets, a type of graph convolutional neural network, to be best suited for temporal BOLD auto-regression, closely followed by linear models. Chebnets demonstrated an increase in performance with increasing amounts of data, with no complete saturation at 9 h of training data. Good generalization to other kinds of video stimuli and to resting-state data marked the Chebnets’ ability to capture intrinsic brain dynamics rather than only stimulus-specific autocorrelation patterns. Significant subject specificity was found at short prediction time lags. The Chebnets were found to capture lower frequencies at longer prediction time lags, and the spatial correlations in predicted dynamics were found to match traditional functional connectivity networks. Overall, these results demonstrate that large individual functional magnetic resonance imaging (fMRI) datasets can be used to efficiently train purely individual auto-regressive models of brain activity, and that massive amounts of individual data are required to do so. The excellent performance of the Chebnets likely reflects their ability to combine spatial and temporal interactions on large time scales at a low complexity cost. The non-linearities of the models did not appear as a key advantage. In fact, surprisingly, linear versions of the Chebnets appeared to outperform the original non-linear ones. Individual temporal auto-regressive models have the potential to improve the predictability of the BOLD signal. This study is based on a massive, publicly-available dataset, which can serve for future benchmarks of individual auto-regressive modeling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">paugam2024benchmark</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A benchmark of individual auto-regressive models in a massive fMRI dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Paugam, Fran\c{c}ois and Pinsard, Basile and Lajoie, Guillaume and Bellec, Pierre}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Imaging Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/imag_a_00228}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2837-6056}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/imag\%5Fa\%5F00228}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://direct.mit.edu/imag/article-pdf/doi/10.1162/imag\_a\_00228/2461525/imag\_a\_00228.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://bioelecmed.biomedcentral.com" rel="external nofollow noopener" target="_blank">BEME</a> </abbr> </div> <div id="berthon2024using" class="col-sm-8"> <div class="title">Using neural biomarkers to personalize dosing of vagus nerve stimulation</div> <div class="author"> Antonin Berthon, Lorenz Wernisch, Myrta Stoukidi, Michael Thornton, Olivier Tessier-Lariviere, and <span class="more-authors" title="click to view 18 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '18 more authors' ? 'Pascal Fortier-Poisson, Jorin Mamen, Max Pinkney, Susannah Lee, Elvijs Sarkans, Luca Annecchino, Ben Appleton, Philip Garsed, Bret Patterson, Samuel Gonshaw, Matjaz Jakopec, Sudhakaran Shunmugam, Tristan Edwards, Aleksi Tukiainen, Joel Jennings, Guillaume Lajoie, Emil Hewage, Oliver Armitage' : '18 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">18 more authors</span> </div> <div class="periodical"> <em>Bioelectronic Medicine</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1186/s42234-024-00147-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">berthon2024using</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using neural biomarkers to personalize dosing of vagus nerve stimulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Berthon, Antonin and Wernisch, Lorenz and Stoukidi, Myrta and Thornton, Michael and Tessier-Lariviere, Olivier and Fortier-Poisson, Pascal and Mamen, Jorin and Pinkney, Max and Lee, Susannah and Sarkans, Elvijs and Annecchino, Luca and Appleton, Ben and Garsed, Philip and Patterson, Bret and Gonshaw, Samuel and Jakopec, Matjaz and Shunmugam, Sudhakaran and Edwards, Tristan and Tukiainen, Aleksi and Jennings, Joel and Lajoie, Guillaume and Hewage, Emil and Armitage, Oliver}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Bioelectronic Medicine}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1186/s42234-024-00147-4}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2332-8886}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1186/s42234-024-00147-4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="mittal2024does" class="col-sm-8"> <div class="title">Does learning the right latent variables necessarily improve in-context learning?</div> <div class="author"> Sarthak Mittal, Eric Elmoznino, Leo Gagnon, Sangnie Bhardwaj, Dhanya Sridhar, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2405.19162" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mittal2024does</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Does learning the right latent variables necessarily improve in-context learning?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Elmoznino, Eric and Gagnon, Leo and Bhardwaj, Sangnie and Sridhar, Dhanya and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2405.19162}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.19162}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="jhu2024amortizing" class="col-sm-8"> <div class="title">Amortizing intractable inference in large language models</div> <div class="author"> Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yoshua Bengio, Nikolay Malkin' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=Ouj6p4ca60" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jhu2024amortizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Amortizing intractable inference in large language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Edward J and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=Ouj6p4ca60}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="pogodin2024synaptic" class="col-sm-8"> <div class="title">Synaptic Weight Distributions Depend on the Geometry of Plasticity</div> <div class="author"> Roman Pogodin, Jonathan Cornford, Arna Ghosh, Gauthier Gidel, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Blake Aaron Richards' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=x5txICnnjC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pogodin2024synaptic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Synaptic Weight Distributions Depend on the Geometry of Plasticity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pogodin, Roman and Cornford, Jonathan and Ghosh, Arna and Gidel, Gauthier and Lajoie, Guillaume and Richards, Blake Aaron}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=x5txICnnjC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="li2024leveraging" class="col-sm-8"> <div class="title">Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency</div> <div class="author"> Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=kNjrhD67LP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024leveraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Tianhong and Bhardwaj, Sangnie and Tian, Yonglong and Zhang, Han and Barber, Jarred and Katabi, Dina and Lajoie, Guillaume and Chang, Huiwen and Krishnan, Dilip}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=kNjrhD67LP}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="hkrishna2024sufficient" class="col-sm-8"> <div class="title">Sufficient conditions for offline reactivation in recurrent neural networks</div> <div class="author"> Nanda H Krishna, Colin Bredenberg, Daniel Levenstein, Blake Aaron Richards, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=RVrINT6MT7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hkrishna2024sufficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sufficient conditions for offline reactivation in recurrent neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krishna, Nanda H and Bredenberg, Colin and Levenstein, Daniel and Richards, Blake Aaron and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=RVrINT6MT7}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="renfalet2024delta-ai" class="col-sm-8"> <div class="title">Delta-AI: Local objectives for amortized inference in sparse graphical models</div> <div class="author"> Jean-Pierre René Falet, Hae Beom Lee, Nikolay Malkin, Chen Sun, Dragos Secrieru, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Dinghuai Zhang, Guillaume Lajoie, Yoshua Bengio' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=LemSSn8htt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">renfalet2024delta-ai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Delta-{AI}: Local objectives for amortized inference in sparse graphical models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Falet, Jean-Pierre Ren{\'e} and Lee, Hae Beom and Malkin, Nikolay and Sun, Chen and Secrieru, Dragos and Zhang, Dinghuai and Lajoie, Guillaume and Bengio, Yoshua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=LemSSn8htt}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="helenaliu2024how" class="col-sm-8"> <div class="title">How connectivity structure shapes rich and lazy learning in neural circuits</div> <div class="author"> Yuhan Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas, Eric Todd SheaBrown, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=slSmYGc8ee" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">helenaliu2024how</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How connectivity structure shapes rich and lazy learning in neural circuits}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yuhan Helena and Baratin, Aristide and Cornford, Jonathan and Mihalas, Stefan and SheaBrown, Eric Todd and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=slSmYGc8ee}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="white2024learning" class="col-sm-8"> <div class="title">Learning and Aligning Structured Random Feature Networks</div> <div class="author"> Vivian White, Muawiz Sajjad Chaudhary, Guy Wolf, <em>Guillaume Lajoie</em>, and Kameron Decker Harris </div> <div class="periodical"> <em>In ICLR 2024 Workshop on Representational Alignment</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=vWhUQXQoFF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">white2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning and Aligning Structured Random Feature Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{White, Vivian and Chaudhary, Muawiz Sajjad and Wolf, Guy and Lajoie, Guillaume and Harris, Kameron Decker}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR 2024 Workshop on Representational Alignment}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=vWhUQXQoFF}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://iopscience.iop.org/journal/1741-2552" rel="external nofollow noopener" target="_blank">JNE</a> </abbr> </div> <div id="wernisch2024online" class="col-sm-8"> <div class="title">Online Bayesian optimization of vagus nerve stimulation</div> <div class="author"> Lorenz Wernisch, Tristan Edwards, Antonin Berthon, Olivier Tessier-Lariviere, Elvijs Sarkans, and <span class="more-authors" title="click to view 19 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '19 more authors' ? 'Myrta Stoukidi, Pascal Fortier-Poisson, Max Pinkney, Michael Thornton, Catherine Hanley, Susannah Lee, Joel Jennings, Ben Appleton, Phillip Garsed, Bret Patterson, Will Buttinger, Samuel Gonshaw, Matjaž Jakopec, Sudhakaran Shunmugam, Jorin Mamen, Aleksi Tukiainen, Guillaume Lajoie, Oliver Armitage, Emil Hewage' : '19 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">19 more authors</span> </div> <div class="periodical"> <em>Journal of Neural Engineering</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1088/1741-2552/ad33ae" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Objective. In bioelectronic medicine, neuromodulation therapies induce neural signals to the brain or organs, modifying their function. Stimulation devices capable of triggering exogenous neural signals using electrical waveforms require a complex and multi-dimensional parameter space to control such waveforms. Determining the best combination of parameters (waveform optimization or dosing) for treating a particular patient’s illness is therefore challenging. Comprehensive parameter searching for an optimal stimulation effect is often infeasible in a clinical setting due to the size of the parameter space. Restricting this space, however, may lead to suboptimal therapeutic results, reduced responder rates, and adverse effects. Approach. As an alternative to a full parameter search, we present a flexible machine learning, data acquisition, and processing framework for optimizing neural stimulation parameters, requiring as few steps as possible using Bayesian optimization. This optimization builds a model of the neural and physiological responses to stimulations, enabling it to optimize stimulation parameters and provide estimates of the accuracy of the response model. The vagus nerve (VN) innervates, among other thoracic and visceral organs, the heart, thus controlling heart rate (HR), making it an ideal candidate for demonstrating the effectiveness of our approach. Main results. The efficacy of our optimization approach was first evaluated on simulated neural responses, then applied to VN stimulation intraoperatively in porcine subjects. Optimization converged quickly on parameters achieving target HRs and optimizing neural B-fiber activations despite high intersubject variability. Significance. An optimized stimulation waveform was achieved in real time with far fewer stimulations than required by alternative optimization strategies, thus minimizing exposure to side effects. Uncertainty estimates helped avoiding stimulations outside a safe range. Our approach shows that a complex set of neural stimulation parameters can be optimized in real-time for a patient to achieve a personalized precision dosing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wernisch2024online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Bayesian optimization of vagus nerve stimulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wernisch, Lorenz and Edwards, Tristan and Berthon, Antonin and Tessier-Lariviere, Olivier and Sarkans, Elvijs and Stoukidi, Myrta and Fortier-Poisson, Pascal and Pinkney, Max and Thornton, Michael and Hanley, Catherine and Lee, Susannah and Jennings, Joel and Appleton, Ben and Garsed, Phillip and Patterson, Bret and Buttinger, Will and Gonshaw, Samuel and Jakopec, Matja\v{z} and Shunmugam, Sudhakaran and Mamen, Jorin and Tukiainen, Aleksi and Lajoie, Guillaume and Armitage, Oliver and Hewage, Emil}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Neural Engineering}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IOP Publishing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{026019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1088/1741-2552/ad33ae}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dx.doi.org/10.1088/1741-2552/ad33ae}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://iopscience.iop.org/journal/1741-2552" rel="external nofollow noopener" target="_blank">JNE</a> </abbr> </div> <div id="mao2024personalized" class="col-sm-8"> <div class="title">Personalized inference for neurostimulation with meta-learning: a case study of vagus nerve stimulation</div> <div class="author"> Ximeng Mao, Yao-Chuan Chang, Stavros Zanos, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>Journal of Neural Engineering</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1088/1741-2552/ad17f4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Objective. Neurostimulation is emerging as treatment for several diseases of the brain and peripheral organs. Due to variability arising from placement of stimulation devices, underlying neuroanatomy and physiological responses to stimulation, it is essential that neurostimulation protocols are personalized to maximize efficacy and safety. Building such personalized protocols would benefit from accumulated information in increasingly large datasets of other individuals’ responses. Approach. To address that need, we propose a meta-learning family of algorithms to conduct few-shot optimization of key fitting parameters of physiological and neural responses in new individuals. While our method is agnostic to neurostimulation setting, here we demonstrate its effectiveness on the problem of physiological modeling of fiber recruitment during vagus nerve stimulation (VNS). Using data from acute VNS experiments, the mapping between amplitudes of stimulus-evoked compound action potentials (eCAPs) and physiological responses, such as heart rate and breathing interval modulation, is inferred. Main results. Using additional synthetic data sets to complement experimental results, we demonstrate that our meta-learning framework is capable of directly modeling the physiology-eCAP relationship for individual subjects with much fewer individually queried data points than standard methods. Significance. Our meta-learning framework is general and can be adapted to many input–response neurostimulation mapping problems. Moreover, this method leverages information from growing data sets of past patients, as a treatment is deployed. It can also be combined with several model types, including regression, Gaussian processes with Bayesian optimization, and beyond.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mao2024personalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalized inference for neurostimulation with meta-learning: a case study of vagus nerve stimulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Ximeng and Chang, Yao-Chuan and Zanos, Stavros and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Neural Engineering}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IOP Publishing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{016004}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1088/1741-2552/ad17f4}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dx.doi.org/10.1088/1741-2552/ad17f4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.nature.com/ncomms/" rel="external nofollow noopener" target="_blank">Nat Commun</a> </abbr> </div> <div id="suárez2024connectome-based" class="col-sm-8"> <div class="title">Connectome-based reservoir computing with the conn2res toolbox</div> <div class="author"> Laura E. Suárez, Agoston Mihalik, Filip Milisav, Kenji Marshall, Mingze Li, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Petra E. Vértes, Guillaume Lajoie, Bratislav Misic' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Nature Communications</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1038/s41467-024-44900-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">suárez2024connectome-based</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Connectome-based reservoir computing with the conn2res toolbox}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su\'{a}rez, Laura E. and Mihalik, Agoston and Milisav, Filip and Marshall, Kenji and Li, Mingze and V\'{e}rtes, Petra E. and Lajoie, Guillaume and Misic, Bratislav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Communications}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41467-024-44900-4}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2041-1723}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1038/s41467-024-44900-4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="nam2024discrete" class="col-sm-8"> <div class="title">Discrete, compositional, and symbolic representations through attractor dynamics</div> <div class="author"> Andrew Nam, Eric Elmoznino, Nikolay Malkin, James McClelland, Yoshua Bengio, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2310.01807" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">nam2024discrete</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discrete, compositional, and symbolic representations through attractor dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nam, Andrew and Elmoznino, Eric and Malkin, Nikolay and McClelland, James and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.01807}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.01807}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://academic.oup.com/nc" rel="external nofollow noopener" target="_blank">NC</a> </abbr> </div> <div id="ji2024sources" class="col-sm-8"> <div class="title">Sources of richness and ineffability for phenomenally conscious states</div> <div class="author"> Xu Ji, Eric Elmoznino, George Deane, Axel Constant, Guillaume Dumas, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Guillaume Lajoie, Jonathan Simon, Yoshua Bengio' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Neuroscience of Consciousness</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/nc/niae001" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Conscious states–state that there is something it is like to be in–seem both rich or full of detail and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function of two individuals relates to improved communicability of their experiences to each other. While our model may not settle all questions relating to the explanatory gap, it makes progress toward a fully physicalist explanation of the richness and ineffability of conscious experience–two important aspects that seem to be part of what makes qualitative character so puzzling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ji2024sources</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sources of richness and ineffability for phenomenally conscious states}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ji, Xu and Elmoznino, Eric and Deane, George and Constant, Axel and Dumas, Guillaume and Lajoie, Guillaume and Simon, Jonathan and Bengio, Yoshua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neuroscience of Consciousness}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{niae001}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/nc/niae001}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2057-2107}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1093/nc/niae001}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://academic.oup.com/nc/article-pdf/2024/1/niae001/60791056/niae001.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="bredenberg2023formalizing" class="col-sm-8"> <div class="title">Formalizing locality for normative synaptic plasticity models</div> <div class="author"> Colin Bredenberg, Ezekiel Williams, Cristina Savin, Blake Richards, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2023/file/120339238f293d4ae53a7167403abc4b-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bredenberg2023formalizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Formalizing locality for normative synaptic plasticity models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bredenberg, Colin and Williams, Ezekiel and Savin, Cristina and Richards, Blake and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5653--5684}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2023/file/120339238f293d4ae53a7167403abc4b-Paper-Conference.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="azabou2023unified" class="col-sm-8"> <div class="title">A Unified, Scalable Framework for Neural Population Decoding</div> <div class="author"> Mehdi Azabou, Vinam Arora, Venkataramana Ganesh, Ximeng Mao, Santosh Nachimuthu, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Michael Mendelson, Blake Richards, Matthew Perich, Guillaume Lajoie, Eva Dyer' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2023/file/8ca113d122584f12a6727341aaf58887-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azabou2023unified</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Unified, Scalable Framework for Neural Population Decoding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azabou, Mehdi and Arora, Vinam and Ganesh, Venkataramana and Mao, Ximeng and Nachimuthu, Santosh and Mendelson, Michael and Richards, Blake and Perich, Matthew and Lajoie, Guillaume and Dyer, Eva}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{44937--44956}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2023/file/8ca113d122584f12a6727341aaf58887-Paper-Conference.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="williams2023flexible" class="col-sm-8"> <div class="title">Flexible Phase Dynamics for Bio-Plausible Contrastive Learning</div> <div class="author"> Ezekiel Williams, Colin Bredenberg, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, 23–29 jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v202/williams23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v202/williams23a/williams23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many learning algorithms used as normative models in neuroscience or as candidate approaches for learning on neuromorphic chips learn by contrasting one set of network states with another. These Contrastive Learning (CL) algorithms are traditionally implemented with rigid, temporally non-local, and periodic learning dynamics, that could limit the range of physical systems capable of harnessing CL. In this study, we build on recent work exploring how CL might be implemented by biological or neurmorphic systems and show that this form of learning can be made temporally local, and can still function even if many of the dynamical requirements of standard training procedures are relaxed. Thanks to a set of general theorems corroborated by numerical experiments across several CL models, our results provide theoretical foundations for the study and development of CL methods for biological and neuromorphic neural networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">williams2023flexible</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flexible Phase Dynamics for Bio-Plausible Contrastive Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Williams, Ezekiel and Bredenberg, Colin and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{23--29 Jul}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pmlr}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{37042--37065}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v202/williams23a.html}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.cell.com/cell-reports-medicine/home" rel="external nofollow noopener" target="_blank">Cell Rep Med</a> </abbr> </div> <div id="bonizzato2023autonomous" class="col-sm-8"> <div class="title">Autonomous optimization of neuroprosthetic stimulation parameters that drive the motor cortex and spinal cord outputs in rats and monkeys</div> <div class="author"> Marco Bonizzato, Rose Guay Hottin, Sandrine L. Côté, Elena Massai, Léo Choinière, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Uzay Macar, Samuel Laferrière, Parikshat Sirpal, Stephan Quessy, Guillaume Lajoie, Marina Martinez, Numa Dancause' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Cell Reports Medicine</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.xcrm.2023.101008" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bonizzato2023autonomous</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Autonomous optimization of neuroprosthetic stimulation parameters that drive the motor cortex and spinal cord outputs in rats and monkeys}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonizzato, Marco and Guay Hottin, Rose and C\ot\'{e}, Sandrine L. and Massai, Elena and Choini\`{e}re, L\'{e}o and Macar, Uzay and Laferri\`{e}re, Samuel and Sirpal, Parikshat and Quessy, Stephan and Lajoie, Guillaume and Martinez, Marina and Dancause, Numa}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Cell Reports Medicine}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier BV}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101008}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.xcrm.2023.101008}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2666-3791}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1016/j.xcrm.2023.101008}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://journals.sagepub.com/home/jcn" rel="external nofollow noopener" target="_blank">J Child Neurol</a> </abbr> </div> <div id="bergeron2023use" class="col-sm-8"> <div class="title">Use of Invasive Brain-Computer Interfaces in Pediatric Neurosurgery: Technical and Ethical Considerations</div> <div class="author"> David Bergeron, Christian Iorio-Morin, Marco Bonizzato, <em>Guillaume Lajoie</em>, Nathalie Orr Gaucher, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? ' Racine, Alexander G. Weil' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Child Neurology</em>, Apr 2023 </div> <div class="periodical"> Pmid: 37116888 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1177/08830738231167736" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Invasive brain-computer interfaces hold promise to alleviate disabilities in individuals with neurologic injury, with fully implantable brain-computer interface systems expected to reach the clinic in the upcoming decade. Children with severe neurologic disabilities, like quadriplegic cerebral palsy or cervical spine trauma, could benefit from this technology. However, they have been excluded from clinical trials of intracortical brain-computer interface to date. In this manuscript, we discuss the ethical considerations related to the use of invasive brain-computer interface in children with severe neurologic disabilities. We first review the technical hardware and software considerations for the application of intracortical brain-computer interface in children. We then discuss ethical issues related to motor brain-computer interface use in pediatric neurosurgery. Finally, based on the input of a multidisciplinary panel of experts in fields related to brain-computer interface (functional and restorative neurosurgery, pediatric neurosurgery, mathematics and artificial intelligence research, neuroengineering, pediatric ethics, and pragmatic ethics), we then formulate initial recommendations regarding the clinical use of invasive brain-computer interfaces in children.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bergeron2023use</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Use of Invasive Brain-Computer Interfaces in Pediatric Neurosurgery: Technical and Ethical Considerations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bergeron, David and Iorio-Morin, Christian and Bonizzato, Marco and Lajoie, Guillaume and Gaucher, Nathalie Orr and \'{E}ric Racine and Weil, Alexander G.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Child Neurology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3-4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{223--238}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/08830738231167736}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1177/08830738231167736}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Pmid: 37116888}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1177/08830738231167736}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.nature.com/natcomputsci" rel="external nofollow noopener" target="_blank">Nat Comp Sci</a> </abbr> </div> <div id="busch2023multi-view" class="col-sm-8"> <div class="title">Multi-view manifold learning of human brain-state trajectories</div> <div class="author"> Erica L. Busch, Jessie Huang, Andrew Benz, Tom Wallenstein, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Guy Wolf, Smita Krishnaswamy, Nicholas B. Turk-Browne' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Nature Computational Science</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1038/s43588-023-00419-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">busch2023multi-view</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-view manifold learning of human brain-state trajectories}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Busch, Erica L. and Huang, Jessie and Benz, Andrew and Wallenstein, Tom and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita and Turk-Browne, Nicholas B.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Computational Science}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{240–253}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s43588-023-00419-0}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2662-8457}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1038/s43588-023-00419-0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="bhardwaj2023steerable" class="col-sm-8"> <div class="title">Steerable Equivariant Representation Learning</div> <div class="author"> Sangnie Bhardwaj, Willie McClinton, Tongzhou Wang, <em>Guillaume Lajoie</em>, Chen Sun, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Phillip Isola, Dilip Krishnan' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2302.11349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">bhardwaj2023steerable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Steerable Equivariant Representation Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bhardwaj, Sangnie and McClinton, Willie and Wang, Tongzhou and Lajoie, Guillaume and Sun, Chen and Isola, Phillip and Krishnan, Dilip}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2302.11349}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2302.11349}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="moudgil2023learning" class="col-sm-8"> <div class="title">Learning to Optimize with Recurrent Hierarchical Transformers</div> <div class="author"> Abhinav Moudgil, Boris Knyazev, <em>Guillaume Lajoie</em>, and Eugene Belilovsky </div> <div class="periodical"> <em>In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=MusMaHCrs2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">moudgil2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Optimize with Recurrent Hierarchical Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moudgil, Abhinav and Knyazev, Boris and Lajoie, Guillaume and Belilovsky, Eugene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=MusMaHCrs2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="payeur2023neural" class="col-sm-8"> <div class="title">Neural manifolds and learning regimes in neural-interface tasks</div> <div class="author"> Alexandre Payeur, Amy L. Orsborn, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>bioRxiv</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2023.03.11.532146" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Neural activity tends to reside on manifolds whose dimension is lower than the dimension of the whole neural state space. Experiments using brain-computer interfaces (BCIs) with microelectrode arrays implanted in the motor cortex of nonhuman primates have provided ways to test whether neural manifolds influence learning-related neural computations. Starting from a learned BCI-controlled motor task, these experiments explored the effect of changing the BCI decoder to implement perturbations that were either “aligned” or not with the pre-existing neural manifold. In a series of studies, researchers found that within-manifold perturbations (WMPs) evoked fast reassociations of existing neural patterns for rapid adaptation, while outside-manifold perturbations (OMPs) triggered a slower adaptation process that led to the emergence of new neural patterns. Together, these findings have been interpreted as suggesting that these different rates of adaptation might be associated with distinct learning mechanisms. Here, we investigated whether gradient-descent learning could alone explain these differences. Using an idealized model that captures the fixed-point dynamics of recurrent neural networks, we uncovered gradient-based learning dynamics consistent with experimental findings. Crucially, this experimental match arose only when the network was initialized in a lazier learning regime, a concept inherited from deep learning theory. A lazy learning regime—in contrast with a rich regime—implies small changes on synaptic strengths throughout learning. For OMPs, these small changes were less effective at increasing performance and could lead to unstable adaptation with a heightened sensitivity to learning rates. For WMPs, they helped reproduce the reassociation mechanism on short adaptation time scales, especially with large input variances. Since gradient descent has many biologically plausible variants, our findings establish lazy gradient-based learning as a plausible mechanism for adaptation under network-level constraints and unify several experimental results from the literature.Competing Interest StatementALO is a scientific advisor for Meta Reality Labs. GL is a scientific advisor for BIOS Health.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">payeur2023neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural manifolds and learning regimes in neural-interface tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Payeur, Alexandre and Orsborn, Amy L. and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2023.03.11.532146}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2023/12/23/2023.03.11.532146}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2023.03.11.532146}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2023/12/23/2023.03.11.532146.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div id="askarihemmat2023lead" class="col-sm-8"> <div class="title">LEAD: Min-Max Optimization from a Physical Perspective</div> <div class="author"> Reyhane Askari Hemmat, Amartya Mitra, <em>Guillaume Lajoie</em>, and Ioannis Mitliagkas </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Mar 2023 </div> <div class="periodical"> Featured Certification </div> <div class="links"> <a href="https://openreview.net/forum?id=vXSsTYs6ZB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">askarihemmat2023lead</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LEAD}: Min-Max Optimization from a Physical Perspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hemmat, Reyhane Askari and Mitra, Amartya and Lajoie, Guillaume and Mitliagkas, Ioannis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=vXSsTYs6ZB}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Featured Certification}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="ghosh2023how" class="col-sm-8"> <div class="title">How gradient estimator variance and bias impact learning in neural networks</div> <div class="author"> Arna Ghosh, Yuhan Helena Liu, <em>Guillaume Lajoie</em>, Konrad Kording, and Blake Aaron Richards </div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=EBC60mxBwyw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghosh2023how</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How gradient estimator variance and bias impact learning in neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghosh, Arna and Liu, Yuhan Helena and Lajoie, Guillaume and Kording, Konrad and Richards, Blake Aaron}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=EBC60mxBwyw}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="davari2023reliability" class="col-sm-8"> <div class="title">Reliability of CKA as a Similarity Measure in Deep Learning</div> <div class="author"> MohammadReza Davari, Stefan Horoi, Amine Natik, <em>Guillaume Lajoie</em>, Guy Wolf, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Eugene Belilovsky' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=8HRvyxc606" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davari2023reliability</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reliability of {CKA} as a Similarity Measure in Deep Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davari, MohammadReza and Horoi, Stefan and Natik, Amine and Lajoie, Guillaume and Wolf, Guy and Belilovsky, Eugene}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=8HRvyxc606}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div id="kalajdzievski2023transfer" class="col-sm-8"> <div class="title">Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer</div> <div class="author"> Damjan Kalajdzievski, Ximeng Mao, Pascal Fortier-Poisson, <em>Guillaume Lajoie</em>, and Blake Aaron Richards </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=kJcwlP7BRs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kalajdzievski2023transfer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kalajdzievski, Damjan and Mao, Ximeng and Fortier-Poisson, Pascal and Lajoie, Guillaume and Richards, Blake Aaron}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=kJcwlP7BRs}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="mittal2022is" class="col-sm-8"> <div class="title">Is a Modular Architecture Enough?</div> <div class="author"> Sarthak Mittal, Yoshua Bengio, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2022/file/b8d1d741f137d9b6ac4f3c1683791e4a-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mittal2022is</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Is a Modular Architecture Enough?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{28747--28760}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/file/b8d1d741f137d9b6ac4f3c1683791e4a-Paper-Conference.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="liu2022accuracy" class="col-sm-8"> <div class="title">Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules</div> <div class="author"> Yuhan Helena Liu, Arna Ghosh, Blake Richards, Eric Shea-Brown, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2022/file/9226f8122feb9c229c1efd9270ce7021-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2022accuracy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yuhan Helena and Ghosh, Arna and Richards, Blake and Shea-Brown, Eric and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{23077--23097}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/file/9226f8122feb9c229c1efd9270ce7021-Paper-Conference.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://elifesciences.org" rel="external nofollow noopener" target="_blank">eLife</a> </abbr> </div> <div id="suarez2022connectomics-based" class="col-sm-8"> <div class="title">A connectomics-based taxonomy of mammals</div> <div class="author"> Laura E Suarez, Yossi Yovel, Martijn P Heuvel, Olaf Sporns, Yaniv Assaf, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Guillaume Lajoie, Bratislav Misic' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>eLife</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.7554/eLife.78635" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Mammalian taxonomies are conventionally defined by morphological traits and genetics. How species differ in terms of neural circuits and whether inter-species differences in neural circuit organization conform to these taxonomies is unknown. The main obstacle to the comparison of neural architectures has been differences in network reconstruction techniques, yielding species-specific connectomes that are not directly comparable to one another. Here, we comprehensively chart connectome organization across the mammalian phylogenetic spectrum using a common reconstruction protocol. We analyse the mammalian MRI (MaMI) data set, a database that encompasses high-resolution ex vivo structural and diffusion MRI scans of 124 species across 12 taxonomic orders and 5 superorders, collected using a unified MRI protocol. We assess similarity between species connectomes using two methods: similarity of Laplacian eigenspectra and similarity of multiscale topological features. We find greater inter-species similarities among species within the same taxonomic order, suggesting that connectome organization reflects established taxonomic relationships defined by morphology and genetics. While all connectomes retain hallmark global features and relative proportions of connection classes, inter-species variation is driven by local regional connectivity profiles. By encoding connectomes into a common frame of reference, these findings establish a foundation for investigating how neural circuits change over phylogeny, forging a link from genes to circuits to behaviour.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">suarez2022connectomics-based</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A connectomics-based taxonomy of mammals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Suarez, Laura E and Yovel, Yossi and van den Heuvel, Martijn P and Sporns, Olaf and Assaf, Yaniv and Lajoie, Guillaume and Misic, Bratislav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{eLife}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{eLife Sciences Publications, Ltd}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e78635}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.7554/eLife.78635}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2050-084x}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.7554/eLife.78635}</span><span class="p">,</span>
  <span class="na">article_type</span> <span class="p">=</span> <span class="s">{journal}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Baker, Chris I and Jbabdi, Saad and Heuer, Katja}</span><span class="p">,</span>
  <span class="na">pub_date</span> <span class="p">=</span> <span class="s">{2022-11-07}</span><span class="p">,</span>
  <span class="na">citation</span> <span class="p">=</span> <span class="s">{eLife 2022;11:e78635}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{connectomics, mammals, taxonomy}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="mittal2022points" class="col-sm-8"> <div class="title">From Points to Functions: Infinite-dimensional Representations in Diffusion Models</div> <div class="author"> Sarthak Mittal, <em>Guillaume Lajoie</em>, Stefan Bauer, and Arash Mehrjou </div> <div class="periodical"> Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2210.13774" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mittal2022points</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Points to Functions: Infinite-dimensional Representations in Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Lajoie, Guillaume and Bauer, Stefan and Mehrjou, Arash}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2210.13774}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2210.13774}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div id="george2022lazy" class="col-sm-8"> <div class="title">Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty</div> <div class="author"> Thomas George, <em>Guillaume Lajoie</em>, and Aristide Baratin </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=lukVf4VrfP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">george2022lazy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{George, Thomas and Lajoie, Guillaume and Baratin, Aristide}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=lukVf4VrfP}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://signalprocessingsociety.org/events/conferences" rel="external nofollow noopener" target="_blank">MLSP</a> </abbr> </div> <div id="huang2022learning" class="col-sm-8"> <div class="title">Learning Shared Neural Manifolds from Multi-Subject FMRI Data</div> <div class="author"> Jessie Huang, Erica Busch, Tom Wallenstein, Michal Gerasimiuk, Andrew Benz, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Guillaume Lajoie, Guy Wolf, Nicholas Turk-Browne, Smita Krishnaswamy' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)</em>, Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/mlsp55214.2022.9943383" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Functional magnetic resonance imaging (fMRI) data is collected in millions of noisy, redundant dimensions. To understand how different brains process the same stimulus, we aim to denoise the fMRI signal via a meaningful embedding space that captures the data’s intrinsic structure as shared across brains. We assume that stimulus-driven responses share latent features common across subjects that are jointly discoverable. Previous approaches to this problem have relied on linear methods like principal component analysis and shared response modeling. We propose a neural network called MRMD-AE (manifold-regularized multiple- decoder, autoencoder) that learns a common embedding from multi-subject fMRI data while retaining the ability to decode individual responses. Our latent common space represents an extensible manifold (where untrained data can be mapped) and improves classification accuracy of stimulus features of unseen timepoints, as well as cross-subject translation of fMRI signals.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang2022learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Shared Neural Manifolds from Multi-Subject FMRI Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Jessie and Busch, Erica and Wallenstein, Tom and Gerasimiuk, Michal and Benz, Andrew and Lajoie, Guillaume and Wolf, Guy and Turk-Browne, Nicholas and Krishnaswamy, Smita}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{01--06}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/mlsp55214.2022.9943383}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2161-0371}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Manifolds;Conferences;Machine learning;Functional magnetic resonance imaging;Signal processing;Brain modeling;Decoding}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="pezeshki2022multi-scale" class="col-sm-8"> <div class="title">Multi-scale Feature Learning Dynamics: Insights for Double Descent</div> <div class="author"> Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Proceedings of the 39th International Conference on Machine Learning</em>, 17–23 jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v162/pezeshki22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v162/pezeshki22a/pezeshki22a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>An intriguing phenomenon that arises from the high-dimensional learning dynamics of neural networks is the phenomenon of “double descent”. The more commonly studied aspect of this phenomenon corresponds to <em>model-wise</em> double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied <em>epoch-wise</em> double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. We study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions describing the generalization error in terms of low-dimensional scalar macroscopic variables. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical simulations where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pezeshki2022multi-scale</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-scale Feature Learning Dynamics: Insights for Double Descent}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pezeshki, Mohammad and Mitra, Amartya and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{17--23 Jul}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pmlr}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17669--17690}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v162/pezeshki22a.html}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="kerg2022neural" class="col-sm-8"> <div class="title">On Neural Architecture Inductive Biases for Relational Tasks</div> <div class="author"> Giancarlo Kerg, Sarthak Mittal, David Rolnick, Yoshua Bengio, Blake Richards, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 17–23 jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2206.05056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">kerg2022neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Neural Architecture Inductive Biases for Relational Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2206.05056}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2206.05056}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.NE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.nature.com/natmachintell/" rel="external nofollow noopener" target="_blank">Nat Mach Intell</a> </abbr> </div> <div id="farrell2022gradient-based" class="col-sm-8"> <div class="title">Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion</div> <div class="author"> Matthew Farrell, Stefano Recanatesi, Timothy Moore, <em>Guillaume Lajoie</em>, and Eric Shea-Brown </div> <div class="periodical"> <em>Nature Machine Intelligence</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1038/s42256-022-00498-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">farrell2022gradient-based</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Farrell, Matthew and Recanatesi, Stefano and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Machine Intelligence}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{564–573}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s42256-022-00498-0}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2522-5839}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1038/s42256-022-00498-0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://journals.plos.org/ploscompbiol/" rel="external nofollow noopener" target="_blank">PLOS Comp Bio</a> </abbr> </div> <div id="puelmatouzel2022performance-gated" class="col-sm-8"> <div class="title">Performance-gated deliberation: A context-adapted strategy in which urgency is opportunity cost</div> <div class="author"> Maximilian Puelma Touzel, Paul Cisek, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>PLOS Computational Biology</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1371/journal.pcbi.1010080" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Finding the right amount of deliberation, between insufficient and excessive, is a hard decision making problem that depends on the value we place on our time. Average-reward, putatively encoded by tonic dopamine, serves in existing reinforcement learning theory as the opportunity cost of time, including deliberation time. Importantly, this cost can itself vary with the environmental context and is not trivial to estimate. Here, we propose how the opportunity cost of deliberation can be estimated adaptively on multiple timescales to account for non-stationary contextual factors. We use it in a simple decision-making heuristic based on average-reward reinforcement learning (AR-RL) that we call Performance-Gated Deliberation (PGD). We propose PGD as a strategy used by animals wherein deliberation cost is implemented directly as urgency, a previously characterized neural signal effectively controlling the speed of the decision-making process. We show PGD outperforms AR-RL solutions in explaining behaviour and urgency of non-human primates in a context-varying random walk prediction task and is consistent with relative performance and urgency in a context-varying random dot motion task. We make readily testable predictions for both neural activity and behaviour.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">puelmatouzel2022performance-gated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Performance-gated deliberation: A context-adapted strategy in which urgency is opportunity cost}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Puelma Touzel, Maximilian and Cisek, Paul and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{PLOS Computational Biology}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Public Library of Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--33}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1371/journal.pcbi.1010080}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1371/journal.pcbi.1010080}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://signalprocessingsociety.org/events/conferences" rel="external nofollow noopener" target="_blank">ICASSP</a> </abbr> </div> <div id="tong2022embedding" class="col-sm-8"> <div class="title">Embedding Signals on Graphs with Unbalanced Diffusion Earth Mover’s Distance</div> <div class="author"> Alexander Tong, Guillaume Huguet, Dennis Shung, Amine Natik, Manik Kuchroo, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Guillaume Lajoie, Guy Wolf, Smita Krishnaswamy' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/icassp43922.2022.9746556" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In modern relational machine learning it is common to encounter large graphs that arise via interactions or similarities between observations in many domains. Further, in many cases the target entities for analysis are actually signals on such graphs. We propose to compare and organize such datasets of graph signals by using an earth mover’s distance (EMD) with a geodesic cost over the underlying graph. Typically, EMD is computed by optimizing over the cost of transporting one probability distribution to another over an underlying metric space. However, this is inefficient when computing the EMD between many signals. Here, we propose an unbalanced graph EMD that efficiently embeds the unbalanced EMD on an underlying graph into an L1 space, whose metric we call unbalanced diffusion earth mover’s distance (UDEMD). Next, we show how this gives distances between graph signals that are robust to noise. Finally, we apply this to organizing patients based on clinical notes, embedding cells modeled as signals on a gene graph, and organizing genes modeled as signals over a large cell graph. In each case, we show that UDEMD-based embeddings find accurate distances that are highly efficient compared to other methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tong2022embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Embedding Signals on Graphs with Unbalanced Diffusion Earth Mover's Distance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tong, Alexander and Huguet, Guillaume and Shung, Dennis and Natik, Amine and Kuchroo, Manik and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5647--5651}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/icassp43922.2022.9746556}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2379-190x}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Earth;Costs;Conferences;Computational modeling;Biological system modeling;Machine learning;Signal processing;Optimal transport;graph signal processing;knowledge graph;graph diffusion}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.ida-society.org" rel="external nofollow noopener" target="_blank">IDA</a> </abbr> </div> <div id="horoi2022exploring" class="col-sm-8"> <div class="title">Exploring the Geometry and Topology of Neural Network Loss Landscapes</div> <div class="author"> Stefan Horoi, Jessie Huang, Bastian Rieck, <em>Guillaume Lajoie</em>, Guy Wolf, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Smita Krishnaswamy' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Intelligent Data Analysis XX</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Recent work has established clear links between the generalization performance of trained neural networks and the geometry of their loss landscape near the local minima to which they converge. This suggests that qualitative and quantitative examination of the loss landscape geometry could yield insights about neural network generalization performance during training. To this end, researchers have proposed visualizing the loss landscape through the use of simple dimensionality reduction techniques. However, such visualization methods have been limited by their linear nature and only capture features in one or two dimensions, thus restricting sampling of the loss landscape to lines or planes. Here, we expand and improve upon these in three ways. First, we present a novel “jump and retrain” procedure for sampling relevant portions of the loss landscape. We show that the resulting sampled data holds more meaningful information about the network’s ability to generalize. Next, we show that non-linear dimensionality reduction of the jump and retrain trajectories via PHATE, a trajectory and manifold-preserving method, allows us to visualize differences between networks that are generalizing well vs poorly. Finally, we combine PHATE trajectories with a computational homology characterization to quantify trajectory differences.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">horoi2022exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the Geometry and Topology of Neural Network Loss Landscapes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horoi, Stefan and Huang, Jessie and Rieck, Bastian and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Intelligent Data Analysis XX}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{171--184}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-01333-1}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bouadi, Tassadit and Fromont, Elisa and H{\"u}llermeier, Eyke}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="deleu2022continuous-time" class="col-sm-8"> <div class="title">Continuous-Time Meta-Learning with Forward Mode Differentiation</div> <div class="author"> Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua Bengio, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Guillaume Lajoie, Pierre-Luc Bacon' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=57PipS27Km" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">deleu2022continuous-time</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Continuous-Time Meta-Learning with Forward Mode Differentiation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Deleu, Tristan and Kanaa, David and Feng, Leo and Kerg, Giancarlo and Bengio, Yoshua and Lajoie, Guillaume and Bacon, Pierre-Luc}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=57PipS27Km}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.frontiersin.org/journals/applied-mathematics-and-statistics" rel="external nofollow noopener" target="_blank">Front AMS</a> </abbr> </div> <div id="10.3389/fams.2022.818799" class="col-sm-8"> <div class="title">On Lyapunov Exponents for RNNs: Understanding Information Propagation Using Dynamical Systems Tools</div> <div class="author"> Ryan Vogt, Maximilian Puelma Touzel, Eli Shlizerman, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>Frontiers in Applied Mathematics and Statistics</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/fams.2022.818799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p></p> <p>Recurrent neural networks (RNNs) have been successfully applied to a variety of problems involving sequential data, but their optimization is sensitive to parameter initialization, architecture, and optimizer hyperparameters. Considering RNNs as dynamical systems, a natural way to capture stability, i.e., the growth and decay over long iterates, are the Lyapunov Exponents (LEs), which form the Lyapunov spectrum. The LEs have a bearing on stability of RNN training dynamics since forward propagation of information is related to the backward propagation of error gradients. LEs measure the asymptotic rates of expansion and contraction of non-linear system trajectories, and generalize stability analysis to the time-varying attractors structuring the non-autonomous dynamics of data-driven RNNs. As a tool to understand and exploit stability of training dynamics, the Lyapunov spectrum fills an existing gap between prescriptive mathematical approaches of limited scope and computationally-expensive empirical approaches. To leverage this tool, we implement an efficient way to compute LEs for RNNs during training, discuss the aspects specific to standard RNN architectures driven by typical sequential datasets, and show that the Lyapunov spectrum can serve as a robust readout of training stability across hyperparameters. With this exposition-oriented contribution, we hope to draw attention to this under-studied, but theoretically grounded tool for understanding training stability in RNNs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.3389/fams.2022.818799</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Lyapunov Exponents for RNNs: Understanding Information Propagation Using Dynamical Systems Tools}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vogt, Ryan and Puelma Touzel, Maximilian and Shlizerman, Eli and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Applied Mathematics and Statistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{Volume 8 - 2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fams.2022.818799}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2297-4687}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.frontiersin.org/journals/applied-mathematics-and-statistics/articles/10.3389/fams.2022.818799}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="gagnon2022clarifying" class="col-sm-8"> <div class="title">Clarifying MCMC-based training of modern EBMs : Contrastive Divergence versus Maximum Likelihood</div> <div class="author"> Léo Gagnon and <em>Guillaume Lajoie</em> </div> <div class="periodical"> May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2202.12176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">gagnon2022clarifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Clarifying MCMC-based training of modern EBMs : Contrastive Divergence versus Maximum Likelihood}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gagnon, L\'{e}o and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2202.12176}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2202.12176}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="mittal2022compositional" class="col-sm-8"> <div class="title">Compositional Attention: Disentangling Search and Retrieval</div> <div class="author"> Sarthak Mittal, Sharath Chandra Raparthy, Irina Rish, Yoshua Bengio, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=IwJPj2MBcIa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mittal2022compositional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compositional Attention: Disentangling Search and Retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Raparthy, Sharath Chandra and Rish, Irina and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=IwJPj2MBcIa}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="davidmárton2022efficient" class="col-sm-8"> <div class="title">Efficient and robust multi-task learning in the brain with modular latent primitives</div> <div class="author"> Christian David Márton, Léo Gagnon, <em>Guillaume Lajoie</em>, and Kanaka Rajan </div> <div class="periodical"> May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2105.14108" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">davidmárton2022efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient and robust multi-task learning in the brain with modular latent primitives}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{M\'{a}rton, Christian David and Gagnon, L\'{e}o and Lajoie, Guillaume and Rajan, Kanaka}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2105.14108}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2105.14108}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="pezeshki2021gradient" class="col-sm-8"> <div class="title">Gradient Starvation: A Learning Proclivity in Neural Networks</div> <div class="author"> Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pezeshki2021gradient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Starvation: A Learning Proclivity in Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pezeshki, Mohammad and Kaba, Oumar and Bengio, Yoshua and Courville, Aaron C and Precup, Doina and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1256--1272}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.nature.com/natmachintell/" rel="external nofollow noopener" target="_blank">Nat Mach Intell</a> </abbr> </div> <div id="suárez2021learning" class="col-sm-8"> <div class="title">Learning function from structure in neuromorphic networks</div> <div class="author"> Laura E. Suárez, Blake A. Richards, <em>Guillaume Lajoie</em>, and Bratislav Misic </div> <div class="periodical"> <em>Nature Machine Intelligence</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1038/s42256-021-00376-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">suárez2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning function from structure in neuromorphic networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su\'{a}rez, Laura E. and Richards, Blake A. and Lajoie, Guillaume and Misic, Bratislav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Machine Intelligence}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{771–786}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s42256-021-00376-1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2522-5839}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1038/s42256-021-00376-1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://direct.mit.edu/neco" rel="external nofollow noopener" target="_blank">Neural Comput</a> </abbr> </div> <div id="abrevaya2021learning" class="col-sm-8"> <div class="title">Learning Brain Dynamics With Coupled Low-Dimensional Nonlinear Oscillators and Deep Recurrent Networks</div> <div class="author"> Germán Abrevaya, Guillaume Dumas, Aleksandr Y. Aravkin, Peng Zheng, Jean-Christophe Gagnon-Audet, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'James Kozloski, Pablo Polosecki, Guillaume Lajoie, David Cox, Silvina Ponce Dawson, Guillermo Cecchi, Irina Rish' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Neural Computation</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/neco_a_01401" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Many natural systems, especially biological ones, exhibit complex multivariate nonlinear dynamical behaviors that can be hard to capture by linear autoregressive models. On the other hand, generic nonlinear models such as deep recurrent neural networks often require large amounts of training data, not always available in domains such as brain imaging; also, they often lack interpretability. Domain knowledge about the types of dynamics typically observed in such systems, such as a certain type of dynamical systems models, could complement purely data-driven techniques by providing a good prior. In this work, we consider a class of ordinary differential equation (ODE) models known as van der Pol (VDP) oscil lators and evaluate their ability to capture a low-dimensional representation of neural activity measured by different brain imaging modalities, such as calcium imaging (CaI) and fMRI, in different living organisms: larval zebrafish, rat, and human. We develop a novel and efficient approach to the nontrivial problem of parameters estimation for a network of coupled dynamical systems from multivariate data and demonstrate that the resulting VDP models are both accurate and interpretable, as VDP’s coupling matrix reveals anatomically meaningful excitatory and inhibitory interactions across different brain subsystems. VDP outperforms linear autoregressive models (VAR) in terms of both the data fit accuracy and the quality of insight provided by the coupling matrices and often tends to generalize better to unseen data when predicting future brain activity, being comparable to and sometimes better than the recurrent neural networks (LSTMs). Finally, we demonstrate that our (generative) VDP model can also serve as a data-augmentation tool leading to marked improvements in predictive accuracy of recurrent neural networks. Thus, our work contributes to both basic and applied dimensions of neuroimaging: gaining scientific insights and improving brain-based predictive models, an area of potentially high practical importance in clinical diagnosis and neurotechnology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">abrevaya2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Brain Dynamics With Coupled Low-Dimensional Nonlinear Oscillators and Deep Recurrent Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Abrevaya, Germ\'{a}n and Dumas, Guillaume and Aravkin, Aleksandr Y. and Zheng, Peng and Gagnon-Audet, Jean-Christophe and Kozloski, James and Polosecki, Pablo and Lajoie, Guillaume and Cox, David and Dawson, Silvina Ponce and Cecchi, Guillermo and Rish, Irina}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neural Computation}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2087--2127}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/neco_a_01401}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0899-7667}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/neco\%5Fa\%5F01401}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://direct.mit.edu/neco/article-pdf/33/8/2087/1930932/neco\_a\_01401.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="ke2021systematic" class="col-sm-8"> <div class="title">Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning</div> <div class="author"> Nan Rosemary Ke, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal ALIAS PARTH GOYAL, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Stefan Bauer, Danilo Jimenez Rezende, Michael Mozer, Yoshua Bengio, Chris Pal' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/8f121ce07d74717e0b1f21d122e04521-Paper-round2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ke2021systematic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ke, Nan Rosemary and Didolkar, Aniket and Mittal, Sarthak and ALIAS PARTH GOYAL, Anirudh Goyal and Lajoie, Guillaume and Bauer, Stefan and Jimenez Rezende, Danilo and Mozer, Michael and Bengio, Yoshua and Pal, Chris}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://datasets-benchmarks-proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/8f121ce07d74717e0b1f21d122e04521-Paper-round2.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Vanschoren, J. and Yeung, S.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neuro.embs.org/" rel="external nofollow noopener" target="_blank">NER</a> </abbr> </div> <div id="tessier-larivière2021pns-gan" class="col-sm-8"> <div class="title">PNS-GAN: Conditional Generation of Peripheral Nerve Signals in the Wavelet Domain via Adversarial Networks</div> <div class="author"> Olivier Tessier-Larivière, Luke Y. Prince, Pascal Fortier-Poisson, Lorenz Wernisch, Oliver Armitage, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Emil Hewage, Guillaume Lajoie, Blake A. Richards' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ner49283.2021.9441284" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Simulated datasets of neural recordings are a crucial tool in neural engineering for testing the ability of decoding algorithms to recover known ground-truth. In this work, we introduce PNS-GAN, a generative adversarial network capable of producing realistic nerve recordings conditioned on physiological biomarkers. PNS-GAN operates in the wavelet domain to preserve both the timing and frequency of neural events with high resolution. PNS-GAN generates sequences of scaleograms from noise using a recurrent neural network and 2D transposed convolution layers. PNS-GAN discriminates over stacks of scaleograms with a network of 3D convolution layers. We find that our generated signal reproduces a number of characteristics of the real signal, including similarity in a canonical time-series feature-space, and contains physiologically related neural events including respiration modulation and similar distributions of afferent and efferent signalling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tessier-larivière2021pns-gan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PNS-GAN: Conditional Generation of Peripheral Nerve Signals in the Wavelet Domain via Adversarial Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tessier-Larivi\`{e}re, Olivier and Prince, Luke Y. and Fortier-Poisson, Pascal and Wernisch, Lorenz and Armitage, Oliver and Hewage, Emil and Lajoie, Guillaume and Richards, Blake A.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{778--782}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ner49283.2021.9441284}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1948-3554}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Wavelet domain;Three-dimensional displays;Recurrent neural networks;Convolution;Neural engineering;Tools;Physiology}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://aistats.org" rel="external nofollow noopener" target="_blank">AISTATS</a> </abbr> </div> <div id="baratin2021implicit" class="col-sm-8"> <div class="title">Implicit Regularization via Neural Feature Alignment</div> <div class="author"> Aristide Baratin, Thomas George, César Laurent, R Devon Hjelm, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pascal Vincent, Simon Lacoste-Julien' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</em>, 13–15 apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v130/baratin21a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://proceedings.mlr.press/v130/baratin21a/baratin21a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a regularization effect induced by a dynamical alignment ofthe neural tangent features introduced by Jacot et al. (2018), along a small number of task-relevant directions. This can be interpreted as a combined mechanism of feature selection and compression. By extrapolating a new analysis of Rademacher complexity bounds for linear models, we motivate and study a heuristic complexity measure that captures this phenomenon, in terms of sequences of tangent kernel classes along optimization paths. The code for our experiments is available as https://github.com/tfjgeorge/ntk_alignment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">baratin2021implicit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Implicit Regularization via Neural Feature Alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Devon Hjelm, R and Lajoie, Guillaume and Vincent, Pascal and Lacoste-Julien, Simon}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{13--15 Apr}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pmlr}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{130}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2269--2277}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v130/baratin21a.html}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Banerjee, Arindam and Fukumizu, Kenji}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.nature.com/ncomms/" rel="external nofollow noopener" target="_blank">Nat Commun</a> </abbr> </div> <div id="recanatesi2021predictive" class="col-sm-8"> <div class="title">Predictive learning as a network mechanism for extracting low-dimensional latent space representations</div> <div class="author"> Stefano Recanatesi, Matthew Farrell, <em>Guillaume Lajoie</em>, Sophie Deneve, Mattia Rigotti, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Eric Shea-Brown' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Nature Communications</em>, Mar 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1038/s41467-021-21696-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">recanatesi2021predictive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Predictive learning as a network mechanism for extracting low-dimensional latent space representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Recanatesi, Stefano and Farrell, Matthew and Lajoie, Guillaume and Deneve, Sophie and Rigotti, Mattia and Shea-Brown, Eric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Communications}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41467-021-21696-1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2041-1723}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1038/s41467-021-21696-1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="mittal2020learning" class="col-sm-8"> <div class="title">Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules</div> <div class="author"> Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Guillaume Lajoie, Michael Mozer, Yoshua Bengio' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 37th International Conference on Machine Learning</em>, 13–18 jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v119/mittal20a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://proceedings.mlr.press/v119/mittal20a/mittal20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what’s directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and the current reportable short-term memory, such as how the phrase ‘peanut butter and ...’ will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emphbidirectional information flow can improve results over strong baselines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mittal2020learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{13--18 Jul}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 37th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pmlr}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{119}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6972--6986}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v119/mittal20a.html}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{III, Hal Daum\'{e} and Singh, Aarti}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="geadah2020advantages" class="col-sm-8"> <div class="title">Advantages of biologically-inspired adaptive neural activation in RNNs during learning</div> <div class="author"> Victor Geadah, Giancarlo Kerg, Stefan Horoi, Guy Wolf, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> 13–18 jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2006.12253" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">geadah2020advantages</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advantages of biologically-inspired adaptive neural activation in RNNs during learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Geadah, Victor and Kerg, Giancarlo and Horoi, Stefan and Wolf, Guy and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2006.12253}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2006.12253}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#08573b"> <a href="https://www.embs.org/tnsre/" rel="external nofollow noopener" target="_blank">TNSRE</a> </abbr> </div> <div id="laferriere2020hierarchical" class="col-sm-8"> <div class="title">Hierarchical Bayesian Optimization of Spatiotemporal Neurostimulations for Targeted Motor Outputs</div> <div class="author"> Samuel Laferrière, Marco Bonizzato, Sandrine L. Côté, Numa Dancause, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>IEEE Transactions on Neural Systems and Rehabilitation Engineering</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/tnsre.2020.2987001" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The development of neurostimulation techniques to evoke motor patterns is an active area of research. It serves as a crucial experimental tool to probe computation in neural circuits, and has applications in neuroprostheses used to aid recovery of motor function after stroke or injury to the nervous system. There are two important challenges when designing algorithms to unveil and control neurostimulation-to-motor correspondences, thereby linking spatiotemporal patterns of neural stimulation to muscle activation: (1) the exploration of motor maps needs to be fast and efficient (exhaustive search is to be avoided for clinical and experimental reasons) (2) online learning needs to be flexible enough to deal with noise and occasional spurious responses. We propose a stimulation search algorithm to address these issues, and demonstrate its efficacy with experiments in the motor cortex (M1) of a non-human primate model. Our solution is a novel iterative process using Bayesian Optimization via Gaussian Processes on a hierarchy of increasingly complex signal spaces. We show that our algorithm can successfully and rapidly learn correspondences between complex stimulation patterns and evoked muscle activation patterns, where standard approaches fail. Importantly, we uncover nonlinear circuit-level computations in M1 that would have been difficult to identify using conventional mapping techniques.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">laferriere2020hierarchical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Bayesian Optimization of Spatiotemporal Neurostimulations for Targeted Motor Outputs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Laferri\`{e}re, Samuel and Bonizzato, Marco and C\{o}t\'{e}, Sandrine L. and Dancause, Numa and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Neural Systems and Rehabilitation Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1452--1460}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/tnsre.2020.2987001}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1558-0210}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Electromyography;Muscles;Optimization;Electrodes;Bayes methods;Gaussian processes;Spatiotemporal phenomena;Neural engineering;machine learning algorithms;optimization methods;motor cortex (M1);neural stimulation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="horoi2020internal" class="col-sm-8"> <div class="title">Internal representation dynamics and geometry in recurrent neural networks</div> <div class="author"> Stefan Horoi, <em>Guillaume Lajoie</em>, and Guy Wolf </div> <div class="periodical"> Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2001.03255" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">horoi2020internal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Internal representation dynamics and geometry in recurrent neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horoi, Stefan and Lajoie, Guillaume and Wolf, Guy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2001.03255}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2001.03255}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="kerg2020untangling" class="col-sm-8"> <div class="title">Untangling tradeoffs between recurrence and self-attention in artificial neural networks</div> <div class="author"> Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal ALIAS PARTH GOYAL, Kyle Goyette, Yoshua Bengio, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guillaume Lajoie' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kerg2020untangling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Untangling tradeoffs between recurrence and self-attention in artificial neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kerg, Giancarlo and Kanuparthi, Bhargav and ALIAS PARTH GOYAL, Anirudh Goyal and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{19443--19454}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.caiac.ca/en/conferences" rel="external nofollow noopener" target="_blank">CCAI</a> </abbr> </div> <div id="horoi2020low-dimensional" class="col-sm-8"> <div class="title">Low-Dimensional Dynamics of Encoding and Learning in Recurrent Neural Networks</div> <div class="author"> Stefan Horoi, Victor Geadah, Guy Wolf, and <em>Guillaume Lajoie</em> </div> <div class="periodical"> <em>In Advances in Artificial Intelligence</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In this paper, we use dimensionality reduction techniques to study how a recurrent neural network (RNN) processes and encodes information in the context of a classification task, and we explain our findings using tools from dynamical systems theory. We observe that internal representations develop a task-relevant structure as soon as significant information is provided as input and this structure remains for some time even if we let the dynamics drift. However, the structure is only interpretable by the final classifying layer at the fixed time step for which the network was trained. We measure that throughout the training, the recurrent weights matrix is modified so that the resulting dynamical system associated with the network’s neural activations evolves into a non-trivial attractor, reminiscent of neural oscillations in the brain. Our findings suggest that RNNs change their internal dynamics throughout training so that information is stored in low-dimensional cycles, rather than in high-dimensional clusters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">horoi2020low-dimensional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Low-Dimensional Dynamics of Encoding and Learning in Recurrent Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horoi, Stefan and Geadah, Victor and Wolf, Guy and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{276--282}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-030-47358-7}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Goutte, Cyril and Zhu, Xiaodan}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="recanatesi2019dimensionality" class="col-sm-8"> <div class="title">Dimensionality compression and expansion in Deep Neural Networks</div> <div class="author"> Stefano Recanatesi, Matthew Farrell, Madhu Advani, Timothy Moore, <em>Guillaume Lajoie</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Eric Shea-Brown' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/1906.00443" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">recanatesi2019dimensionality</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dimensionality compression and expansion in Deep Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Recanatesi, Stefano and Farrell, Matthew and Advani, Madhu and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/1906.00443}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1906.00443}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30000"> <a href="https://biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a> </abbr> </div> <div id="bogaard2019cortical" class="col-sm-8"> <div class="title">Cortical network mechanisms of anodal and cathodal transcranial direct current stimulation in awake primates</div> <div class="author"> Andrew R. Bogaard, <em>Guillaume Lajoie</em>, Hayley Boyd, Andrew Morse, Stavros Zanos, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Eberhard E. Fetz' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>bioRxiv</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/516260" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Transcranial direct current stimulation (tDCS) is a non-invasive neuromodulation technique that is widely used to stimulate the sensorimotor cortex, and yet the mechanism by which it influences the natural activity of cortical networks is still under debate. Here, we characterize the effects of anodal and cathodal tDCS on underlying neurons in active macaque sensorimotor cortex across a range of doses. We find changes in spike rates that are sensitive to both current intensity and polarity, behavioral state, and that are cell-type specific. At high currents, effects persist after the offset of stimulation, and the spatiotemporal activity associated with motor activity of the contralateral limb, measured by dynamics of neural ensembles, are altered. These data suggest that tDCS induces reproducible and noticeable changes in cortical neuron activity and support the theory that it affects brain activity through a combination of single neuron polarization and network interactions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bogaard2019cortical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cortical network mechanisms of anodal and cathodal transcranial direct current stimulation in awake primates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bogaard, Andrew R. and Lajoie, Guillaume and Boyd, Hayley and Morse, Andrew and Zanos, Stavros and Fetz, Eberhard E.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/516260}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2019/07/08/516260}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{516260}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.biorxiv.org/content/early/2019/07/08/516260.full.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="kerg2019non-normal" class="col-sm-8"> <div class="title">Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics</div> <div class="author"> Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yoshua Bengio, Guillaume Lajoie' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper%5Ffiles/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kerg2019non-normal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kerg, Giancarlo and Goyette, Kyle and Puelma Touzel, Maximilian and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and Lajoie, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textquotesingle Alch\'{e}-Buc, F. and Fox, E. and Garnett, R.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <hr> <p>For older publications, visit <a href="https://scholar.google.com/citations?user=ifu_7_0AAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lajoie Group. Powered by <a href="https://jekyllrb.com/" rel="external nofollow noopener" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" rel="external nofollow noopener" target="_blank">GitHub Pages</a>. Last updated: June 11, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> </body> </html>